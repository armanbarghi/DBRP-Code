{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import torch\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Union, List, Tuple, Optional, Dict\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "import cProfile, pstats\n",
    "\n",
    "from core.env.scene_manager import SceneManager\n",
    "from core.env.scene_manager import show_valid_score_map, draw_dependency_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "phi: 0.480 | uniform size: (31, 31)\n",
      "Manipulator at [49, 49]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 880x440 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "phi = 0.5\n",
    "num_objects = 5\n",
    "grid_size = (100, 100)\n",
    "\n",
    "env = SceneManager(\n",
    "\tmode='stationary', num_objects=num_objects, \n",
    "\tgrid_size=grid_size, phi=phi,\n",
    "\tterminal_cost=True, verbose=1\n",
    ")\n",
    "env.reset(use_stack=False, use_sides=False)\n",
    "initial_scene, target_scene = env.initial_x.clone(), env.target_x.clone()\n",
    "# with open(f'example_phi{phi}_n{num_objects}.pkl', 'rb') as f:\n",
    "# \tscenes = pickle.load(f)\n",
    "# \tinitial_scene = scenes[\"initial\"].to(torch.long)\n",
    "# \ttarget_scene = scenes[\"target\"].to(torch.long)\n",
    "env.reset(initial_scene, target_scene)\n",
    "env.render(show_manipulator=True)\n",
    "# show_valid_score_map(env, obj=0)\n",
    "# draw_dependency_graph(env, fig_size=(3, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Planners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq\n",
    "from typing import Optional\n",
    "from core.env.scene_manager import (\n",
    "\tIndices, copy_state, state_to_hashable,\n",
    "    get_object_below, get_object_above, get_object_base, build_parent_of\n",
    ")\n",
    "\n",
    "def env_cost(env, actions, initial_scene, target_scene, log=True):\n",
    "\tenv.reset(initial_scene, target_scene)\n",
    "\tif actions is None:\n",
    "\t\treturn None\n",
    "\t\n",
    "\tep_cost = 0\n",
    "\tfor action in actions:\n",
    "\t\tep_cost += env.step(action, log=log)[0]\n",
    "\tif log:\n",
    "\t\tprint(f'episode cost: {ep_cost:.3f}')\n",
    "\tenv.reset(initial_scene, target_scene)\n",
    "\treturn ep_cost\n",
    "\n",
    "def evaluate_alg(env, alg, initial_scene, target_scene, num_runs=1, **kwargs):\n",
    "\tplan = None\n",
    "\tsteps, costs, elapsed_time = [], [], []\n",
    "\tbest_cost = np.inf\n",
    "\n",
    "\tprint(f\"--------{alg.__name__}--------\")\n",
    "\tif num_runs > 1:\n",
    "\t\tpbar = tqdm(total=num_runs, desc=f\"Evaluating {alg.__name__}\", unit=\"run\")\n",
    "\t\n",
    "\tfor _ in range(num_runs):\n",
    "\t\tenv.reset(initial_scene, target_scene)\n",
    "\t\tcost = None\n",
    "\t\tplan_i, steps_i, elapsed_time_i = alg(env).solve(**kwargs)\n",
    "\t\tif plan_i:\n",
    "\t\t\tcost = env_cost(env, plan_i, initial_scene, target_scene, log=False)\n",
    "\t\t\tif cost < best_cost:\n",
    "\t\t\t\tplan = plan_i\n",
    "\t\t\tcosts.append(cost)\n",
    "\t\t\tsteps.append(steps_i)\n",
    "\t\t\telapsed_time.append(elapsed_time_i)\n",
    "\n",
    "\t\tif num_runs > 1:\n",
    "\t\t\tpbar.update(1)\n",
    "\t\t\tpbar.set_postfix(cost=cost, steps=steps_i, elapsed_time=elapsed_time_i)\n",
    "\t\telse:\n",
    "\t\t\telapsed_time = elapsed_time_i\n",
    "\t\t\tsteps = steps_i\n",
    "\t\n",
    "\tif num_runs > 1:\n",
    "\t\tpbar.close()\n",
    "\t\tprint(f'mean cost: {np.mean(costs):.2f} | mean elapsed_time: {np.mean(elapsed_time):.3f}s | mean steps: {np.mean(steps):.2f}')\n",
    "\telse:\n",
    "\t\tprint(f'plan: {plan}')\n",
    "\t\tprint(f'elapsed_time: {elapsed_time:.3f}s')\n",
    "\t\tprint(f'steps: {steps}')\n",
    "\n",
    "\t\tif plan is not None:\n",
    "\t\t\tenv_cost(env, plan, initial_scene, target_scene)\n",
    "\n",
    "\tenv.reset(initial_scene, target_scene)\n",
    "\treturn plan\n",
    "\n",
    "def reconstruct_path(node):\n",
    "    path = []\n",
    "    while node.parent is not None:\n",
    "        path.append(node.action)\n",
    "        node = node.parent\n",
    "    path.reverse()\n",
    "    return path\n",
    "\n",
    "class BaseSearch:\n",
    "\tdef __init__(self, env, node_class):\n",
    "\t\t\"\"\"\n",
    "\t\tBase class for search algorithms.\n",
    "\t\t:param env: The environment in which the search is performed.\n",
    "\t\t:param node_class: The class used for representing nodes in the search.\n",
    "\t\t\"\"\"\n",
    "\t\tself.env = env\n",
    "\t\tself.node_class = node_class  # Generalized node class\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labbe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------Labbe--------\n",
      "plan: [31538, 4758, 48035, 26703, 18371, 21686, 17591]\n",
      "elapsed_time: 0.229s\n",
      "steps: 23\n",
      "Moved 3 to: [15 18] | cost: 0.717 | done: False\n",
      "Moved 0 to: [47 38] | cost: 1.009 | done: False\n",
      "Moved 4 to: [80 15] | cost: 0.938 | done: False\n",
      "Moved 2 to: [66 83] | cost: 1.001 | done: False\n",
      "Moved 1 to: [83 51] | cost: 1.071 | done: False\n",
      "Moved 2 to: [16 66] | cost: 1.090 | done: False\n",
      "Moved 1 to: [75 71] | cost: 1.105 | done: True\n",
      "episode cost: 6.933\n",
      "--------Labbe_S--------\n",
      "plan: [31538, 4758, 48035, 11, 17591, 21686]\n",
      "elapsed_time: 0.103s\n",
      "steps: 17\n",
      "Moved 3 to: [15 18] | cost: 0.717 | done: False\n",
      "Moved 0 to: [47 38] | cost: 1.009 | done: False\n",
      "Moved 4 to: [80 15] | cost: 0.938 | done: False\n",
      "Stacked 2 -> 4 | cost: 1.581 | done: False\n",
      "Moved 1 to: [75 71] | cost: 1.320 | done: False\n",
      "Moved 2 to: [16 66] | cost: 1.584 | done: True\n",
      "episode cost: 7.150\n",
      "--------Labbe_S--------\n",
      "plan: [31538, 4758, 48035, 8, 17591, 21686]\n",
      "elapsed_time: 0.094s\n",
      "steps: 18\n",
      "Moved 3 to: [15 18] | cost: 0.717 | done: False\n",
      "Moved 0 to: [47 38] | cost: 1.009 | done: False\n",
      "Moved 4 to: [80 15] | cost: 0.938 | done: False\n",
      "Stacked 2 -> 0 | cost: 1.440 | done: False\n",
      "Moved 1 to: [75 71] | cost: 0.947 | done: False\n",
      "Moved 2 to: [16 66] | cost: 1.054 | done: True\n",
      "episode cost: 6.105\n"
     ]
    }
   ],
   "source": [
    "class LabbeNode:\n",
    "\tnode_counter = 0  # Static variable to assign unique IDs to each node\n",
    "\n",
    "\tdef __init__(self, \n",
    "\t\t\tstate: Dict[str, torch.Tensor], \n",
    "\t\t\tremaining_objs: List[int], \n",
    "\t\t\tobj: Optional[int]=None, \n",
    "\t\t\tparent: Optional[int]=None, \n",
    "\t\t\taction: Optional[int]=None, \n",
    "\t\t\tc: float=1., \n",
    "\t\t\tdepth: int=0\n",
    "\t\t):\n",
    "\t\tself.state = state\n",
    "\t\tself.obj = obj\n",
    "\t\tself.parent = parent\n",
    "\t\tself.action = action\n",
    "\t\tself.children = {}\n",
    "\t\tself.n = 0\n",
    "\t\tself.w = 0.0\n",
    "\t\tself.c = c\n",
    "\t\tself.remaining_objs = remaining_objs\n",
    "\t\tself.depth = depth\n",
    "\t\t\n",
    "\t\t# Assign a unique ID to each node\n",
    "\t\tself.id = LabbeNode.node_counter\n",
    "\t\tLabbeNode.node_counter += 1\n",
    "\n",
    "\tdef get_state(self) -> Dict[str, torch.Tensor]:\n",
    "\t\treturn copy_state(self.state)\n",
    "\n",
    "\tdef is_fully_expanded(self) -> bool:\n",
    "\t\treturn len(self.remaining_objs) == 0\n",
    "\n",
    "\tdef ucb(self) -> float:\n",
    "\t\texpected_value = self.w / self.n\n",
    "\t\texploration_term = self.c * np.sqrt(2 * np.log(self.parent.n) / self.n)\n",
    "\n",
    "\t\treturn expected_value + exploration_term\n",
    "\n",
    "class LabbeMCTS(BaseSearch):\n",
    "\tdef __init__(self, env: 'SceneManager'):\n",
    "\t\tsuper().__init__(env, LabbeNode)\n",
    "\n",
    "\tdef get_remaining_objs(self, state: Dict[str, torch.Tensor]) -> List[int]:\n",
    "\t\traise NotImplementedError\n",
    "\n",
    "\tdef evaluate_state(self, state: Dict[str, torch.Tensor]) -> float:\n",
    "\t\traise NotImplementedError\n",
    "\n",
    "\tdef get_action_move_obj_away(self, k: int) -> Optional[int]:\n",
    "\t\traise NotImplementedError\n",
    "\n",
    "\tdef get_motion(self, k: int) -> Optional[int]:\n",
    "\t\traise NotImplementedError\n",
    "\n",
    "\tdef select(self, node):\n",
    "\t\t# Accesses child nodes for best selection\n",
    "\t\treturn max(node.children.values(), key=lambda child: child.ucb())\n",
    "\n",
    "\tdef expand(self, node):\n",
    "\t\t# Prevents further expansion if no actions remain\n",
    "\t\tif node.is_fully_expanded():\n",
    "\t\t\treturn None\n",
    "\n",
    "\t\t# If the node is too deep, we stop expanding to avoid infinite loops\n",
    "\t\tif node.depth > 2*self.env.N + 2:\n",
    "\t\t\treturn None\n",
    "\n",
    "\t\taction = self.get_motion(node.remaining_objs.pop())\n",
    "\t\tif action is None:\n",
    "\t\t\treturn self.expand(node)\n",
    "\n",
    "\t\taction_type, start_obj, target_obj, coord = self.env.decode_action(action)\n",
    "\n",
    "\t\t# Continue expanding if the last changed obj is the same as the current obj\n",
    "\t\t# if node.obj == start_obj:\n",
    "\t\t# \treturn self.expand(node)\n",
    "\n",
    "\t\t_, child_state = self.env._step(action_type, start_obj, target_obj, coord)\n",
    "\n",
    "\t\tif torch.equal(child_state['current'], node.state['current']):\n",
    "\t\t\traise ValueError('State has not changed')\n",
    "\n",
    "\t\tchild_node = self.node_class(\n",
    "\t\t\tstate=child_state, \n",
    "\t\t\tremaining_objs=self.get_remaining_objs(child_state), \n",
    "\t\t\tobj=start_obj, \n",
    "\t\t\tparent=node, \n",
    "\t\t\taction=action, \n",
    "\t\t\tc=node.c,\n",
    "\t\t\tdepth=node.depth+1\n",
    "\t\t)\n",
    "\t\tnode.children[action] = child_node\n",
    "\n",
    "\t\treturn child_node\n",
    "\n",
    "\tdef backup_search(self, node, value: float):\n",
    "\t\twhile node is not None:\n",
    "\t\t\tnode.n += 1\n",
    "\t\t\tnode.w += value\n",
    "\t\t\tnode = node.parent\n",
    "\n",
    "\tdef print_tree(self, node, depth: int=0):\n",
    "\t\t# Print the current node with indentation to show its depth in the tree\n",
    "\t\tindent = \"    \" * depth  # Four spaces per level of depth\n",
    "\t\tif node.id == 0:\n",
    "\t\t\tprint(f\"Root | Visits: {node.n} | Value: {node.w:.2f}\")\n",
    "\t\telse:\n",
    "\t\t\tprint(f\"{indent}ID: {node.id} | Action: {node.action} | \"\n",
    "\t\t\t\t\tf\"Visits: {node.n} | Value: {node.w:.2f}\")\n",
    "\n",
    "\t\t# Sort children by value estimate\n",
    "\t\tchildren = sorted(node.children.values(), key=lambda child: child.w / child.n if child.n > 0 else float('inf'))\n",
    "\n",
    "\t\t# Recurse on children\n",
    "\t\tfor child in children:\n",
    "\t\t\tself.print_tree(child, depth + 1)\n",
    "\n",
    "\tdef loop(self):\n",
    "\t\tnode = self.root_node\n",
    "\n",
    "\t\t# Selection\n",
    "\t\twhile node.is_fully_expanded():\n",
    "\t\t\tnode = self.select(node)\n",
    "\n",
    "\t\t# Expansion\n",
    "\t\tself.env.set_state(node.get_state())\n",
    "\t\tchild_node = self.expand(node)\n",
    "\t\tif child_node is not None:\n",
    "\t\t\tnode = child_node\n",
    "\t\t\tif self.env.is_terminal_state(node.state):\n",
    "\t\t\t\tself.terminal_node = node\n",
    "\t\t\t\treturn\n",
    "\t\t\tvalue = self.evaluate_state(node.state)\n",
    "\t\telse:\n",
    "\t\t\t# It means it fully expanded\n",
    "\t\t\twhile len(node.children) == 0 and node.parent:\n",
    "\t\t\t\tnode.parent.children.pop(node.action)\n",
    "\t\t\t\tnode = node.parent\n",
    "\t\t\tvalue = 0\n",
    "\n",
    "\t\t# Backpropagation\n",
    "\t\tself.backup_search(node, value)\n",
    "\n",
    "\tdef _solve(self, c: float=0.1, verbose: int=0, time_limit: int=1000):\n",
    "\t\tstart_time = time.time()\n",
    "\t\tself.terminal_node = None\n",
    "\t\tLabbeNode.node_counter = 0\n",
    "\t\tsteps = 0\n",
    "\n",
    "\t\tif self.env.is_terminal_state():\n",
    "\t\t\tprint('The initial scene is already in the target state.')\n",
    "\t\t\treturn [], steps, time.time()-start_time\n",
    "\n",
    "\t\troot_state = self.env.get_state()\n",
    "\t\tself.root_node = self.node_class(\n",
    "\t\t\tstate=root_state, \n",
    "\t\t\tremaining_objs=self.get_remaining_objs(root_state), \n",
    "\t\t\tc=c\n",
    "\t\t)\n",
    "\n",
    "\t\tif verbose > 0:\n",
    "\t\t\tpbar = tqdm(total=None, unit='iterations')\n",
    "\n",
    "\t\twhile self.terminal_node is None:\n",
    "\t\t\t# Check if the elapsed time has exceeded the limit\n",
    "\t\t\tif time.time()-start_time > time_limit:\n",
    "\t\t\t\tif verbose > 0:\n",
    "\t\t\t\t\tpbar.close() # type: ignore\n",
    "\t\t\t\treturn None, steps, time.time()-start_time\n",
    "\n",
    "\t\t\tsteps += 1\n",
    "\t\t\tself.loop()\n",
    "\t\t\tif verbose > 0:\n",
    "\t\t\t\tpbar.update(1) # type: ignore\n",
    "\t\t\t\n",
    "\t\t\tif len(self.root_node.children) == 0:\n",
    "\t\t\t\tprint('No more actions to expand for root node')\n",
    "\t\t\t\tif verbose > 0:\n",
    "\t\t\t\t\tpbar.close() # type: ignore\n",
    "\t\t\t\treturn None, steps, time.time()-start_time\n",
    "\n",
    "\t\tif verbose > 0:\n",
    "\t\t\tpbar.close() # type: ignore\n",
    "\n",
    "\t\treturn reconstruct_path(self.terminal_node), steps, time.time()-start_time\n",
    "\n",
    "class Labbe(LabbeMCTS):\n",
    "\tdef get_remaining_objs(self, state: Dict[str, torch.Tensor]) -> List[int]:\n",
    "\t\t\"\"\"\n",
    "\t\tObjects whose current center ≠ target center.\n",
    "\t\t\"\"\"\n",
    "\t\tcurrent_x, target_x = state['current'], self.env.target_x\n",
    "\n",
    "\t\t# All current vs. target centers: [N,2]\n",
    "\t\tcur_centers = current_x[:, Indices.COORD]\n",
    "\t\ttgt_centers = target_x[:, Indices.COORD]\n",
    "\n",
    "\t\t# Base condition for objs that should be on the table\n",
    "\t\tsatisfied = (cur_centers == tgt_centers).all(dim=1)  # [N]\n",
    "\n",
    "\t\t# Remaining = those not satisfied\n",
    "\t\trem = torch.nonzero(~satisfied, as_tuple=False).view(-1)  # [R]\n",
    "\n",
    "\t\tif rem.numel() == 0:\n",
    "\t\t\treturn []\n",
    "\n",
    "\t\t# Shuffle the remaining indices\n",
    "\t\tperm = torch.randperm(rem.size(0))\n",
    "\t\treturn rem[perm].tolist()\n",
    "\n",
    "\tdef evaluate_state(self, state: Dict[str, torch.Tensor]) -> float:\n",
    "\t\t\"\"\"\n",
    "\t\tFraction of objects exactly at their target centers.\n",
    "\t\t\"\"\"\n",
    "\t\tcurrent_x, target_x = state['current'], self.env.target_x\n",
    "\n",
    "\t\tcur_centers = current_x[:, Indices.COORD]       \n",
    "\t\ttgt_centers = target_x[:, Indices.COORD]  \n",
    "\n",
    "\t\tsatisfied = (cur_centers == tgt_centers).all(dim=1)  # [N]\n",
    "\t\treturn satisfied.float().mean().item()\n",
    "\n",
    "\tdef get_action_move_obj_away(self, k: int) -> Optional[int]:\n",
    "\t\t\"\"\"\n",
    "\t\tMoves object k to it's target position,\n",
    "\t\to.w. moves it randomly.\n",
    "\t\t\"\"\"\n",
    "\t\t# Is target position free?\n",
    "\t\tTK = self.env.target_x[k, Indices.COORD]\n",
    "\t\tif not self.env.is_invalid_center(TK, k):\n",
    "\t\t\treturn self.env.encode_move(k, TK)\n",
    "\n",
    "\t\t# Move away k\n",
    "\t\tfree_positions = self.env.get_empty_positions(ref_obj=k, n=1)\n",
    "\t\tif free_positions.numel() > 0:\n",
    "\t\t\t# move to a random position\n",
    "\t\t\treturn self.env.encode_move(k, free_positions[0])\n",
    "\t\treturn None\n",
    "\n",
    "\tdef get_motion(self, k: int) -> Optional[int]:\n",
    "\t\t\"\"\"\n",
    "\t\tMove object k to its target position if it was free,\n",
    "\t\to.w. move away one of its blocking objects\n",
    "\t\t\"\"\"\n",
    "\t\tCK = self.env.current_x[k, Indices.COORD]\n",
    "\t\tTK = self.env.target_x[k, Indices.COORD]\n",
    "\t\tif torch.equal(TK, CK):\n",
    "\t\t\traise ValueError(f'The obj {k} is already in its target position')\n",
    "\t\t\n",
    "\t\t# Is target position free?\n",
    "\t\tif not self.env.is_invalid_center(TK, k):\n",
    "\t\t\treturn self.env.encode_move(k, TK)\n",
    "\n",
    "\t\t# Move away one of its blocking objects\n",
    "\t\toccupants = self.env.find_blocking_objects(k)\n",
    "\t\tif len(occupants) == 0:\n",
    "\t\t\traise ValueError('No obj is occupying the target position')\n",
    "\t\tfor j in occupants:\n",
    "\t\t\taction_away = self.get_action_move_obj_away(j)\n",
    "\t\t\tif action_away is not None:\n",
    "\t\t\t\treturn action_away\n",
    "\t\treturn None\n",
    "\n",
    "\tdef solve(self, c: float=0.1, verbose: int=0, time_limit: int=1000):\n",
    "\t\tif torch.sum(self.env.initial_x[:, Indices.RELATION]) > 0:\n",
    "\t\t\traise ValueError('Initial scene has stacks in Non-stack mode')\n",
    "\t\tif torch.sum(self.env.current_x[:, Indices.RELATION]) > 0:\n",
    "\t\t\traise ValueError('Current scene has stacks in Non-stack mode')\n",
    "\t\tif torch.sum(self.env.target_x[:, Indices.RELATION]) > 0:\n",
    "\t\t\traise ValueError('Target scene has stacks in Non-stack mode')\n",
    "\t\treturn self._solve(c, verbose, time_limit)\n",
    "\n",
    "class Labbe_S(LabbeMCTS):\n",
    "\tdef get_remaining_objs(self, state: Dict[str, torch.Tensor]) -> List[int]:\n",
    "\t\t\"\"\"\n",
    "\t\t• Objects whose aren't stacked on their target objects.\n",
    "\t\t• Base objects whose current center ≠ target center.\n",
    "\t\t\"\"\"\n",
    "\t\tcurrent_x, target_x = state['current'], self.env.target_x\n",
    "\n",
    "\t\t# Build parent‐of maps once\n",
    "\t\tcur_parent = build_parent_of(current_x)\n",
    "\n",
    "\t\t# Stacking condition for objs that should be stacked\n",
    "\t\tcond_stacked = (self.tgt_parent >= 0) & (cur_parent == self.tgt_parent)\n",
    "\n",
    "\t\t# Base condition for objs that should be on the table\n",
    "\t\tcur_centers = current_x[:, Indices.COORD]\n",
    "\t\ttgt_centers = target_x[:, Indices.COORD]\n",
    "\t\tbase_match = (cur_centers == tgt_centers).all(dim=1)  # [N]\n",
    "\t\tcond_base    = (self.tgt_parent < 0) & (cur_parent < 0) & base_match\n",
    "\n",
    "\t\t# Satisfied = either stacking OK or base OK\n",
    "\t\tsatisfied = cond_stacked | cond_base       # [N]\n",
    "\n",
    "\t\t# Remaining = those not satisfied\n",
    "\t\trem = torch.nonzero(~satisfied, as_tuple=False).view(-1)  # [R]\n",
    "\n",
    "\t\tif rem.numel() == 0:\n",
    "\t\t\treturn []\n",
    "\n",
    "\t\t# Shuffle the remaining indices\n",
    "\t\tperm = torch.randperm(rem.size(0))\n",
    "\t\treturn rem[perm].tolist()\n",
    "\n",
    "\tdef evaluate_state(self, state: Dict[str, torch.Tensor]) -> float:\n",
    "\t\t\"\"\"\n",
    "\t\tFraction of nodes satisfying the same two conditions:\n",
    "\t\t• stacked correctly, or\n",
    "\t\t• matched centers of base objects.\n",
    "\t\t\"\"\"\n",
    "\t\tcurrent_x, target_x = state['current'], self.env.target_x\n",
    "\n",
    "\t\tcur_parent = build_parent_of(current_x)\n",
    "\n",
    "\t\tcur_centers = current_x[:, Indices.COORD]\n",
    "\t\ttgt_centers = target_x[:, Indices.COORD]\n",
    "\t\tbase_match = (cur_centers == tgt_centers).all(dim=1)  # [N]\n",
    "\n",
    "\t\tcond_stacked = (self.tgt_parent >= 0) & (cur_parent == self.tgt_parent)\n",
    "\t\tcond_base    = (self.tgt_parent < 0) & (cur_parent < 0) & base_match\n",
    "\n",
    "\t\tsatisfied = cond_stacked | cond_base\n",
    "\t\treturn satisfied.float().mean().item()\n",
    "\n",
    "\tdef get_action_move_obj_away(self, k: int) -> Optional[int]:\n",
    "\t\t\"\"\"\n",
    "\t\tMoves object k to it's target position or target stack,\n",
    "\t\to.w. moves it randomly or stacks it randomly.\n",
    "\t\t\"\"\"\n",
    "\t\t# Non-empty objects cannot be moved in static_stack mode\n",
    "\t\tif self.static_stack:\n",
    "\t\t\tj = get_object_above(self.env.current_x, k)\n",
    "\t\t\tif j is not None:\n",
    "\t\t\t\treturn None\n",
    "\n",
    "\t\ti = get_object_below(self.env.target_x, k)\n",
    "\t\tif i is not None:\n",
    "\t\t\t# Is target object empty?\n",
    "\t\t\tif get_object_above(self.env.current_x, i) is None:\n",
    "\t\t\t\treturn self.env.encode_stack(k, i)\n",
    "\t\telse:\n",
    "\t\t\t# Is target position free?\n",
    "\t\t\tTK = self.env.target_x[k, Indices.COORD]\n",
    "\t\t\tif not self.env.is_invalid_center(TK, k):\n",
    "\t\t\t\treturn self.env.encode_move(k, TK)\n",
    "\n",
    "\t\t# Move away k\n",
    "\t\tfree_positions = self.env.get_empty_positions(ref_obj=k, n=1)\n",
    "\t\tfree_objects = self.env.get_empty_objs(ref_obj=k, n=1)\n",
    "\t\tif 0 in free_objects and self.env.current_x[:, Indices.RELATION][:, 0].any():\n",
    "\t\t\tprint('shit--')\n",
    "\n",
    "\t\tif len(free_objects) > 0 and free_positions.numel() > 0:\n",
    "\t\t\tif np.random.rand() < 0.5:\n",
    "\t\t\t\t# move to a random position\n",
    "\t\t\t\treturn self.env.encode_move(k, free_positions[0])\n",
    "\t\t\telse:\n",
    "\t\t\t\t# stack on a random object\n",
    "\t\t\t\treturn self.env.encode_stack(k, free_objects[0])\n",
    "\t\telif len(free_objects) == 0 and free_positions.numel() > 0:\n",
    "\t\t\t# move to a random position\n",
    "\t\t\treturn self.env.encode_move(k, free_positions[0])\n",
    "\t\telif free_positions.numel() == 0 and len(free_objects) > 0:\n",
    "\t\t\t# stack on a random object\n",
    "\t\t\treturn self.env.encode_stack(k, free_objects[0])\n",
    "\t\treturn None\n",
    "\n",
    "\tdef get_motion(self, k: int) -> Optional[int]:\n",
    "\t\t\"\"\"\n",
    "\t\tMoves object k to it's target position or target stack,\n",
    "\t\to.w. move away one of it's blocking objects.\n",
    "\t\t\"\"\"\n",
    "\t\t# Non-empty objects cannot be moved in static_stack mode\n",
    "\t\tif self.static_stack:\n",
    "\t\t\tj = get_object_above(self.env.current_x, k)\n",
    "\t\t\tif j is not None:\n",
    "\t\t\t\treturn self.get_action_move_obj_away(j)\n",
    "\n",
    "\t\ti = get_object_below(self.env.target_x, k)\n",
    "\t\tif i is not None:\n",
    "\t\t\t# Is target object empty?\n",
    "\t\t\tj = get_object_above(self.env.current_x, i)\n",
    "\t\t\tif j is not None:\n",
    "\t\t\t\treturn self.get_action_move_obj_away(j)\n",
    "\t\t\treturn self.env.encode_stack(k, i)\n",
    "\n",
    "\t\tCK = self.env.current_x[k, Indices.COORD]\n",
    "\t\tTK = self.env.target_x[k, Indices.COORD]\n",
    "\t\tif torch.equal(TK, CK):\n",
    "\t\t\tj = get_object_below(self.env.current_x, k)\n",
    "\t\t\tif j is None:\n",
    "\t\t\t\traise ValueError(f'The obj {k} is already in its target position')\n",
    "\n",
    "\t\t\tif self.static_stack:\n",
    "\t\t\t\treturn self.get_action_move_obj_away(k)\n",
    "\t\t\telse:\n",
    "\t\t\t\tj = get_object_base(self.env.current_x, j)\n",
    "\t\t\t\treturn self.get_action_move_obj_away(j)\n",
    "\n",
    "\t\t# Is target position free?\n",
    "\t\tif not self.env.is_invalid_center(TK, k):\n",
    "\t\t\treturn self.env.encode_move(k, TK)\n",
    "\n",
    "\t\t# Move away one of its blocking objects\n",
    "\t\tblockers = self.env.find_blocking_objects(k)\n",
    "\t\tif len(blockers) == 0:\n",
    "\t\t\traise ValueError(f'No obj is blocking the target position of {k}')\n",
    "\t\tfor j in blockers:\n",
    "\t\t\taction_away = self.get_action_move_obj_away(j)\n",
    "\t\t\tif action_away is not None:\n",
    "\t\t\t\treturn action_away\n",
    "\t\treturn None\n",
    "\n",
    "\tdef solve(self, c: float=0.1, static_stack: bool=False, verbose: int=0, time_limit: int=1000):\n",
    "\t\tself.static_stack = static_stack\n",
    "\t\tself.env.static_stack = static_stack\n",
    "\t\tself.tgt_parent = build_parent_of(self.env.target_x)\n",
    "\t\treturn self._solve(c, verbose, time_limit)\n",
    "\n",
    "evaluate_alg(env, Labbe, initial_scene, target_scene, num_runs=1, c=0.1, time_limit=20);\n",
    "evaluate_alg(env, Labbe_S, initial_scene, target_scene, num_runs=1, c=0.1, time_limit=20, static_stack=True);\n",
    "evaluate_alg(env, Labbe_S, initial_scene, target_scene, num_runs=1, c=0.1, time_limit=20, static_stack=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sorp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MctsNode:\n",
    "\tnode_counter = 0  # Static variable to assign unique IDs to each node\n",
    "\n",
    "\tdef __init__(self, state, valid_actions, parent=None, action=None, cost=0.0, cost_to_come=0.0, c=1, depth=0):\n",
    "\t\tself.state = state\n",
    "\t\tself.parent = parent\n",
    "\t\tself.action = action\n",
    "\t\tself.children = {}\n",
    "\t\tself.n = 0\n",
    "\t\t# self.w = 0\n",
    "\t\tself.w = np.inf\n",
    "\t\tself.c = c\n",
    "\t\tself.cost = cost\n",
    "\t\tself.cum_cost = cost_to_come\n",
    "\t\tself.unexpanded_actions = valid_actions\n",
    "\t\tself.depth = depth\n",
    "\t\t\n",
    "\t\t# Assign a unique ID to each node\n",
    "\t\tself.id = MctsNode.node_counter\n",
    "\t\tMctsNode.node_counter += 1\n",
    "\n",
    "\tdef get_state(self):\n",
    "\t\treturn copy_state(self.state)\n",
    "\n",
    "\tdef is_fully_expanded(self):\n",
    "\t\treturn len(self.unexpanded_actions) == 0\n",
    "\n",
    "\tdef uct(self, c_min=0, c_max=1):\n",
    "\t\tn = self.n\n",
    "\n",
    "\t\t# expected_value = ( (self.cum_cost + self.w / n) - c_min ) / (c_max - c_min)\n",
    "\t\texpected_value = ( (self.cum_cost + self.w) - c_min ) / (c_max - c_min)\n",
    "\t\texploration_term = np.sqrt(2 * np.log(self.parent.n) / n)\n",
    "\n",
    "\t\treturn expected_value - self.c * exploration_term  # Minimization form\n",
    "\n",
    "class Sorp(BaseSearch):\n",
    "\tdef __init__(self, env):\n",
    "\t\tsuper().__init__(env, MctsNode)\n",
    "\t\n",
    "\tdef get_remaining_objs(self, state: dict) -> list:\n",
    "\t\t\"\"\"\n",
    "\t\t• Objects whose aren't stacked on their target objects.\n",
    "\t\t• Base objects whose current center ≠ target center.\n",
    "\t\t\"\"\"\n",
    "\t\tcurrent_x, target_x = state['current'], self.env.target_x\n",
    "\n",
    "\t\t# Build parent‐of maps once\n",
    "\t\tcur_parent = build_parent_of(current_x)\n",
    "\n",
    "\t\t# Stacking condition for objs that should be stacked\n",
    "\t\tcond_stacked = (self.tgt_parent >= 0) & (cur_parent == self.tgt_parent)\n",
    "\n",
    "\t\t# Base condition for objs that should be on the table\n",
    "\t\tcur_centers = current_x[:, Indices.COORD]\n",
    "\t\ttgt_centers = target_x[:, Indices.COORD]\n",
    "\t\tbase_match = (cur_centers == tgt_centers).all(dim=1)  # [N]\n",
    "\t\tcond_base    = (self.tgt_parent < 0) & (cur_parent < 0) & base_match\n",
    "\n",
    "\t\t# Satisfied = either stacking OK or base OK\n",
    "\t\tsatisfied = cond_stacked | cond_base       # [N]\n",
    "\n",
    "\t\t# Remaining = those not satisfied\n",
    "\t\trem = torch.nonzero(~satisfied, as_tuple=False).view(-1)  # [R]\n",
    "\n",
    "\t\tif rem.numel() == 0:\n",
    "\t\t\treturn []\n",
    "\n",
    "\t\t# Shuffle the remaining indices\n",
    "\t\tperm = torch.randperm(rem.size(0))\n",
    "\t\treturn rem[perm].tolist()\n",
    "\n",
    "\tdef get_valid_actions(self, state: dict) -> list:\n",
    "\t\t\"\"\"\n",
    "\t\t...\n",
    "\t\t\"\"\"\n",
    "\t\t# restore the env to this state\n",
    "\t\tself.env.set_state(copy_state(state))\n",
    "\n",
    "\t\trem = self.get_remaining_objs(state)          # Python list of ints\n",
    "\t\tif len(rem) == 0:\n",
    "\t\t\treturn []\n",
    "\n",
    "\t\t# Which k are allowed (static_stack skips non‐empty actors)\n",
    "\t\tif self.static_stack:\n",
    "\t\t\trem = torch.tensor(rem, dtype=torch.long)\n",
    "\t\t\trel     = self.env.current_x[:, Indices.RELATION]\n",
    "\t\t\tempty_k = ~rel.any(dim=0)                  # True if k has no one on top\n",
    "\t\t\tmask = empty_k[rem]\n",
    "\t\t\tks = rem[mask].tolist()\n",
    "\t\telse:\n",
    "\t\t\tks = rem\n",
    "\n",
    "\t\tvalid_actions = []\n",
    "\t\tstack_nums = max(int(0.6 * self.num_buffers), 1)\n",
    "\n",
    "\t\tfor k in ks:\n",
    "\t\t\tvalid_stacks = []\n",
    "\t\t\tobjs = self.env.get_empty_objs(ref_obj=k, n=stack_nums)\n",
    "\t\t\tif len(objs) > 0:\n",
    "\t\t\t\tM      = len(objs)\n",
    "\t\t\t\tstarts = torch.full((M,), k, dtype=torch.long)\n",
    "\t\t\t\ttargets= torch.tensor(objs, dtype=torch.long)\n",
    "\t\t\t\tvalid_stacks = self.env.encode_stack(starts, targets).tolist()\n",
    "\n",
    "\t\t\tvalid_moves = []\n",
    "\t\t\tcoords = self.env.get_empty_positions_with_target(\n",
    "\t\t\t\tref_obj=k,\n",
    "\t\t\t\tn=self.num_buffers-len(valid_stacks),\n",
    "\t\t\t\tsort=self.score_sorting\n",
    "\t\t\t)\n",
    "\n",
    "\t\t\tif coords.numel() > 0:\n",
    "\t\t\t\tM      = coords.size(0)\n",
    "\t\t\t\tstarts = torch.full((M,), k, dtype=torch.long)\n",
    "\t\t\t\tvalid_moves  = self.env.encode_move(starts, coords).tolist()\n",
    "\n",
    "\t\t\tvalid_actions += valid_stacks + valid_moves\n",
    "\n",
    "\t\treturn valid_actions\n",
    "\n",
    "\tdef select(self, node):\n",
    "\t\t# Accesses child nodes for best selection\n",
    "\t\tif self.c_min == np.inf or self.c_max == self.c_min:\n",
    "\t\t\treturn min(node.children.values(), key=lambda child: child.uct())\n",
    "\t\telse:\n",
    "\t\t\treturn min(node.children.values(), key=lambda child: child.uct(self.c_min, self.c_max))\n",
    "\n",
    "\tdef expand(self, node):\n",
    "\t\tif node.is_fully_expanded():\n",
    "\t\t\treturn None  # Prevents further expansion if no actions remain\n",
    "\n",
    "\t\taction = node.unexpanded_actions.pop()\n",
    "\t\tself.env.set_state(node.get_state())\n",
    "\t\taction_type, start_obj, target_obj, coordinates = self.env.decode_action(action)\n",
    "\n",
    "\t\t# Continue expanding if the last changed node is the same as the current node\n",
    "\t\tif node.action is not None:\n",
    "\t\t\t_, last_obj, _, _ = self.env.decode_action(node.action)\n",
    "\t\t\tif last_obj == start_obj:\n",
    "\t\t\t\treturn self.expand(node)\n",
    "\n",
    "\t\tcost, child_state = self.env._step(action_type, start_obj, target_obj, coordinates)\n",
    "\n",
    "\t\t# Continue expanding if the state hasn't changed\n",
    "\t\tif torch.equal(child_state['current'], node.state['current']):\n",
    "\t\t\traise ValueError('State has not changed')\n",
    "\n",
    "\t\tchild_node = self.node_class(\n",
    "\t\t\tstate=child_state, \n",
    "\t\t\tvalid_actions=self.get_valid_actions(self.env.get_state()), \n",
    "\t\t\tparent=node, \n",
    "\t\t\taction=action, \n",
    "\t\t\tcost=cost, \n",
    "\t\t\tcost_to_come=cost+node.cum_cost,\n",
    "\t\t\tc=node.c, \n",
    "\t\t\tdepth=node.depth+1\n",
    "\t\t)\n",
    "\t\tnode.children[action] = child_node\n",
    "\n",
    "\t\treturn child_node\n",
    "\n",
    "\tdef rollout_one(self, node):\n",
    "\t\tcosts = 0\n",
    "\n",
    "\t\tself.env.set_state(node.get_state())\n",
    "\t\tsim_time_limit = (self.time_limit - time.time() + self.start_time) / 4\n",
    "\t\tfeasible_path, steps, _ = Labbe_S(self.env).solve(time_limit=sim_time_limit, static_stack=self.static_stack)\n",
    "\t\tself.env.set_state(node.get_state())\n",
    "\n",
    "\t\tif feasible_path:\n",
    "\t\t\tfor i, action in enumerate(feasible_path):\n",
    "\t\t\t\tcost, child_state = self.env.step(action)\n",
    "\t\t\t\tcosts += cost\n",
    "\t\t\t\tif i == 0:\n",
    "\t\t\t\t\t# remove action from the node's unexpanded actions\n",
    "\t\t\t\t\tif action in node.unexpanded_actions:\n",
    "\t\t\t\t\t\tnode.unexpanded_actions.remove(action)\n",
    "\t\t\t\t\t# add the new child to the node\n",
    "\t\t\t\t\tchild_node = self.node_class(\n",
    "\t\t\t\t\t\tstate=child_state, \n",
    "\t\t\t\t\t\tvalid_actions=self.get_valid_actions(child_state), \n",
    "\t\t\t\t\t\tparent=node, \n",
    "\t\t\t\t\t\taction=action, \n",
    "\t\t\t\t\t\tcost=cost, \n",
    "\t\t\t\t\t\tcost_to_come=cost+node.cum_cost,\n",
    "\t\t\t\t\t\tc=node.c, \n",
    "\t\t\t\t\t\tdepth=node.depth+1\n",
    "\t\t\t\t\t)\n",
    "\t\t\t\t\tnode.children[action] = child_node\n",
    "\t\t\t\t\tnode = child_node\n",
    "\n",
    "\t\treturn costs, steps, feasible_path, node\n",
    "\n",
    "\tdef rollout_whole(self, node):\n",
    "\t\tcosts = 0\n",
    "\n",
    "\t\tself.env.set_state(node.get_state())\n",
    "\t\tsim_time_limit = (self.time_limit - time.time() + self.start_time) / 4\n",
    "\t\tfeasible_path, steps, _ = Labbe_S(self.env).solve(time_limit=sim_time_limit, static_stack=self.static_stack)\n",
    "\t\tself.env.set_state(node.get_state())\n",
    "\n",
    "\t\tif feasible_path:\n",
    "\t\t\tfor action in feasible_path:\n",
    "\t\t\t\tcost, child_state = self.env.step(action)\n",
    "\t\t\t\tcosts += cost\n",
    "\t\t\t\t# remove action from the node's unexpanded actions\n",
    "\t\t\t\tif action in node.unexpanded_actions:\n",
    "\t\t\t\t\tnode.unexpanded_actions.remove(action)\n",
    "\t\t\t\t# add the new child to the node\n",
    "\t\t\t\tchild_node = self.node_class(\n",
    "\t\t\t\t\tstate=child_state, \n",
    "\t\t\t\t\tvalid_actions=self.get_valid_actions(child_state), \n",
    "\t\t\t\t\tparent=node, \n",
    "\t\t\t\t\taction=action, \n",
    "\t\t\t\t\tcost=cost, \n",
    "\t\t\t\t\tcost_to_come=cost+node.cum_cost,\n",
    "\t\t\t\t\tc=node.c, \n",
    "\t\t\t\t\tdepth=node.depth+1\n",
    "\t\t\t\t)\n",
    "\t\t\t\tnode.children[action] = child_node\n",
    "\t\t\t\tnode = child_node\n",
    "\n",
    "\t\treturn costs, steps, feasible_path, node\n",
    "\n",
    "\tdef rollout(self, node):\n",
    "\t\tif self.one_step:\n",
    "\t\t\treturn self.rollout_one(node)\n",
    "\t\telse:\n",
    "\t\t\treturn self.rollout_whole(node)\n",
    "\n",
    "\tdef backup_search(self, node, value):\n",
    "\t\twhile node is not None:\n",
    "\t\t\tnode.n += 1\n",
    "\t\t\t# node.w += value\n",
    "\t\t\tnode.w = min(node.w, value)\n",
    "\t\t\tvalue += node.cost\n",
    "\t\t\tnode = node.parent\n",
    "\n",
    "\tdef print_tree(self, node, depth=0, max_depth=float('inf'), ter=False):\n",
    "\t\tif depth >= max_depth:\n",
    "\t\t\treturn\n",
    "\n",
    "\t\t# Print the current node with indentation to show its depth in the tree\n",
    "\t\tindent = \"    \" * depth  # Four spaces per level of depth\n",
    "\t\tif node.id == 0:\n",
    "\t\t\tprint(f\"Root | n: {node.n} | w: {node.w:.2f}\")\n",
    "\t\telse:\n",
    "\t\t\tn = node.n\n",
    "\t\t\tif self.c_max == self.c_min:\n",
    "\t\t\t\t# expected_value = node.cum_cost + node.w / n\n",
    "\t\t\t\texpected_value = node.cum_cost + node.w\n",
    "\t\t\telse:\n",
    "\t\t\t\t# expected_value = ( (node.cum_cost + node.w / n) - self.c_min ) / (self.c_max - self.c_min)\n",
    "\t\t\t\texpected_value = ( (node.cum_cost + node.w) - self.c_min ) / (self.c_max - self.c_min)\n",
    "\t\t\texploration_term = np.sqrt(2 * np.log(node.parent.n) / n)\n",
    "\n",
    "\t\t\tprint(f\"{indent}ID: {node.id} | a: {node.action} | c: {node.cost:.2f} | ctc: {node.cum_cost:.2f} | \"\n",
    "\t\t\t\t\tf\"n: {node.n} | w: {node.w:.2f} | expe: {expected_value:.4f} | expl: {exploration_term:.4f}\")\n",
    "\n",
    "\t\t# Sort children by w estimate\n",
    "\t\tif ter:\n",
    "\t\t\taccepted_children = [child for child in node.children.values()]\n",
    "\t\t\tchildren = sorted(accepted_children, key=lambda child: child.w)\n",
    "\t\telse:\n",
    "\t\t\tchildren = sorted(node.children.values(), key=lambda child: child.w)\n",
    "\n",
    "\t\t# Recurse on children\n",
    "\t\tfor child in children:\n",
    "\t\t\tself.print_tree(child, depth + 1, max_depth, ter)\n",
    "\n",
    "\tdef find_best_path(self):\n",
    "\t\t# self.print_tree(self.root_node, max_depth=5)\n",
    "\t\tif self.best_plan is None or self.best_plan[1] is None:\n",
    "\t\t\treturn None\n",
    "\t\treturn reconstruct_path(self.best_plan[0]) + self.best_plan[1]\n",
    "\n",
    "\tdef loop(self):\n",
    "\t\tnode = self.root_node\n",
    "\n",
    "\t\t# Selection\n",
    "\t\tself.env.set_state(node.get_state())\n",
    "\t\twhile node.is_fully_expanded() and not self.env.is_terminal_state():\n",
    "\t\t\tnode = self.select(node)\n",
    "\t\t\tself.env.set_state(node.get_state())\n",
    "\n",
    "\t\t# Expansion\n",
    "\t\tif not self.env.is_terminal_state():\n",
    "\t\t\tchild_node = self.expand(node)\n",
    "\t\t\tif child_node is None:\n",
    "\t\t\t\twhile len(node.children) == 0 and node.is_fully_expanded():\n",
    "\t\t\t\t\tnode.parent.children.pop(node.action)\n",
    "\t\t\t\t\tnode = node.parent\n",
    "\t\t\t\t\tprint('oooooooooooooooooooo laaaaaaaa laaaaaaaaaaaaa')\n",
    "\t\t\t\treturn 1\n",
    "\t\t\tnode = child_node\n",
    "\n",
    "\t\t# Simulation (Rollout)\n",
    "\t\tself.env.set_state(node.get_state())\n",
    "\t\tsteps = 0\n",
    "\t\tif self.env.is_terminal_state():\n",
    "\t\t\tvalue = 0\n",
    "\t\telse:\n",
    "\t\t\tc_rollout, steps, feasible_plan, child_node = self.rollout(node)\n",
    "\n",
    "\t\t\tif feasible_plan is None:\n",
    "\t\t\t\tif node.parent is None:\n",
    "\t\t\t\t\treturn -1\n",
    "\t\t\t\tnode.parent.children.pop(node.action)\n",
    "\t\t\t\tnode = node.parent\n",
    "\t\t\t\twhile len(node.children) == 0 and node.is_fully_expanded():\n",
    "\t\t\t\t\tnode.parent.children.pop(node.action)\n",
    "\t\t\t\t\tnode = node.parent\n",
    "\t\t\t\t\tif node.parent is None:\n",
    "\t\t\t\t\t\treturn -1\n",
    "\t\t\t\treturn steps\n",
    "\n",
    "\t\t\tnew_cost = c_rollout + node.cum_cost\n",
    "\t\t\tself.c_max = max(self.c_max, new_cost)\n",
    "\t\t\tif new_cost < self.c_min:\n",
    "\t\t\t\tself.best_plan = (node, feasible_plan)\n",
    "\t\t\t\tself.c_min = new_cost\n",
    "\t\t\t\n",
    "\t\t\tnode = child_node\n",
    "\t\t\tif self.one_step:\n",
    "\t\t\t\tvalue = c_rollout - node.cost\n",
    "\t\t\t\t# value = c_rollout\n",
    "\t\t\telse:\n",
    "\t\t\t\tvalue = 0\n",
    "\n",
    "\t\t# Backpropagation\n",
    "\t\tself.backup_search(node, value)\n",
    "\n",
    "\t\treturn steps\n",
    "\n",
    "\tdef solve(\n",
    "\t\t\tself, iterations: int=1000, \n",
    "\t\t\tnum_buffers: int=4, score_sorting: bool=False,\n",
    "\t\t\tc: float=1, verbose: int=0, \n",
    "\t\t\tone_step: bool=True, static_stack: bool=False, \n",
    "\t\t\ttime_limit: int=1000\n",
    "\t\t):\n",
    "\t\tself.start_time = time.time()\n",
    "\t\tself.static_stack = static_stack\n",
    "\t\tself.score_sorting = score_sorting\n",
    "\t\tself.tgt_parent = build_parent_of(self.env.target_x)\n",
    "\n",
    "\t\tMctsNode.node_counter = 0\n",
    "\t\tself.num_buffers = num_buffers\n",
    "\t\tself.time_limit = time_limit\n",
    "\t\tself.one_step = one_step\n",
    "\t\tself.c_max = -np.inf\n",
    "\t\tself.c_min = np.inf\n",
    "\t\tself.best_plan = None\n",
    "\t\twindow_last_values = []\n",
    "\t\tself.root_node = self.node_class(\n",
    "\t\t\tstate=self.env.get_state(), \n",
    "\t\t\tvalid_actions=self.get_valid_actions(self.env.get_state()), \n",
    "\t\t\tc=c\n",
    "\t\t)\n",
    "\t\t\n",
    "\t\tsteps, iteration = 0, 0\n",
    "\t\tif verbose > 0:\n",
    "\t\t\tpbar = tqdm(total=None, unit='iterations')\n",
    "\t\t\n",
    "\t\twhile iteration < iterations:\n",
    "\t\t\t# Check if the elapsed time has exceeded the limit\n",
    "\t\t\tif time.time()-self.start_time > time_limit:\n",
    "\t\t\t\tif verbose > 0:\n",
    "\t\t\t\t\tpbar.close()\n",
    "\t\t\t\tprint('Time limit exceeded')\n",
    "\t\t\t\treturn self.find_best_path(), steps, time.time()-self.start_time\n",
    "\t\t\t\n",
    "\t\t\titeration += 1\n",
    "\t\t\tstep = self.loop()\n",
    "\t\t\tif step == -1:\n",
    "\t\t\t\treturn self.find_best_path(), steps, time.time()-self.start_time\n",
    "\t\t\t\n",
    "\t\t\tsteps += step\n",
    "\t\t\tif verbose > 0:\n",
    "\t\t\t\tpbar.update(1)\n",
    "\t\t\t\n",
    "\t\t\t# if iteration != 0 and iteration % 10 == 0:\n",
    "\t\t\t# \tv_root = self.root_node.w\n",
    "\t\t\t# \tprint(f'v_root: {v_root:.3f} | c_min: {self.c_min:.3f} | c_max: {self.c_max:.3f}')\n",
    "\t\t\t# \tself.print_tree(self.root_node, max_depth=3)\n",
    "\t\t\t# \twindow_last_values.append(self.c_min)\n",
    "\t\t\t# \tif len(window_last_values) > 5:\n",
    "\t\t\t# \t\twindow_last_values.pop(0)\n",
    "\t\t\t# \t\tif len(set(window_last_values)) == 1:\n",
    "\t\t\t# \t\t\tbreak\n",
    "\n",
    "\t\tif verbose > 0:\n",
    "\t\t\tpbar.close()\n",
    "\n",
    "\t\treturn self.find_best_path(), steps, time.time()-self.start_time\n",
    "\n",
    "# prof = cProfile.Profile()\n",
    "# prof.enable()\n",
    "# evaluate_alg(\n",
    "# \tenv, Sorp, initial_scene, target_scene, \n",
    "# \tnum_runs=1, score_sorting=False,\n",
    "# \titerations=1000, num_buffers=4, one_step=True,\n",
    "# \tc=0.5, time_limit=20, verbose=1\n",
    "# )\n",
    "# prof.disable()\n",
    "# pstats.Stats(prof).sort_stats('tottime').print_stats(30);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AstarNode:\n",
    "\tdef __init__(self, \n",
    "\t\t\tstate: Dict[str, torch.Tensor], \n",
    "\t\t\tparent: Optional[int]=None, \n",
    "\t\t\taction: Optional[int]=None, \n",
    "\t\t\tg_cost: float=0.0, \n",
    "\t\t\th_cost: float=0.0, \n",
    "\t\t\tdepth: int=0\n",
    "\t\t):\n",
    "\t\tself.state = state\n",
    "\t\tself.parent = parent\n",
    "\t\tself.action = action\n",
    "\t\tself.g_cost = g_cost\t# cost-to-come\n",
    "\t\tself.h_cost = h_cost\t# cost-to-go\n",
    "\t\tself.total_cost = self.g_cost + self.h_cost\n",
    "\t\tself.depth = depth\n",
    "\n",
    "\tdef __lt__(self, other):\n",
    "\t\treturn self.total_cost < other.total_cost  # Lower cost first\n",
    "\n",
    "\tdef get_state(self) -> Dict[str, torch.Tensor]:\n",
    "\t\treturn copy_state(self.state)\n",
    "\n",
    "class Astar(BaseSearch):\n",
    "\tdef __init__(self, env: 'SceneManager'):\n",
    "\t\tsuper().__init__(env, AstarNode)\n",
    "\t\n",
    "\tdef get_remaining_objs(self, state: Dict[str, torch.Tensor]) -> List[int]:\n",
    "\t\traise NotImplementedError\n",
    "\t\n",
    "\tdef get_valid_actions(self, state: Dict[str, torch.Tensor]) -> List[int]:\n",
    "\t\traise NotImplementedError\n",
    "\t\n",
    "\tdef evaluate_state(self, state: Dict[str, torch.Tensor]) -> float:\n",
    "\t\traise NotImplementedError\n",
    "\t\n",
    "\tdef _solve(self, time_limit: int=1000):\n",
    "\t\tstart_time = time.time()\n",
    "\t\t\n",
    "\t\tsteps = 0\n",
    "\t\troot_state = self.env.get_state()\n",
    "\t\troot_node = self.node_class(\n",
    "\t\t\tstate=root_state, \n",
    "\t\t\tg_cost=0, \n",
    "\t\t\th_cost=self.evaluate_state(root_state)\n",
    "\t\t)\n",
    "\n",
    "\t\tself.queue = []\n",
    "\t\theapq.heappush(self.queue, root_node)\n",
    "\t\tvisited = {}\n",
    "\n",
    "\t\twhile self.queue:\n",
    "\t\t\tcurrent_node = heapq.heappop(self.queue)\n",
    "\n",
    "\t\t\t# Check if the current node's state matches the target state\n",
    "\t\t\tif self.env.is_terminal_state(current_node.state):\n",
    "\t\t\t\treturn reconstruct_path(current_node), steps, time.time()-start_time\n",
    "\n",
    "\t\t\t# Check if the elapsed time has exceeded the limit\n",
    "\t\t\tif time.time()-start_time > time_limit:\n",
    "\t\t\t\tprint('Time limit exceeded')\n",
    "\t\t\t\treturn None, steps, time.time()-start_time\n",
    "\n",
    "\t\t\tlast_obj = self.env.decode_action(current_node.action)[1] if current_node.action is not None else None\n",
    "\t\t\tfor action in self.get_valid_actions(current_node.state):\n",
    "\t\t\t\taction_type, start_obj, target_obj, coordinates = self.env.decode_action(action)\n",
    "\n",
    "\t\t\t\t# If the last changed obj is the same as the current obj, continue\n",
    "\t\t\t\tif start_obj == last_obj:\n",
    "\t\t\t\t\tcontinue\n",
    "\n",
    "\t\t\t\tsteps += 1\n",
    "\t\t\t\tself.env.set_state(current_node.get_state())\n",
    "\t\t\t\tcost, child_state = self.env._step(action_type, start_obj, target_obj, coordinates)\n",
    "\n",
    "\t\t\t\t# If state hasn't changed, continue\n",
    "\t\t\t\tif torch.equal(child_state['current'], current_node.state['current']):\n",
    "\t\t\t\t\traise ValueError('State has not changed')\n",
    "\n",
    "\t\t\t\tchild_hash = state_to_hashable(child_state)\n",
    "\n",
    "\t\t\t\t# Calculate the accumulated cost for the current path\n",
    "\t\t\t\tnew_g_cost = current_node.g_cost + cost\n",
    "\t\t\t\th_cost = self.evaluate_state(child_state)\n",
    "\t\t\t\tnew_total_cost = new_g_cost + h_cost\n",
    "\n",
    "\t\t\t\t# Retain the node with better cost\n",
    "\t\t\t\tif child_hash not in visited or visited[child_hash] > new_total_cost:\n",
    "\t\t\t\t\tvisited[child_hash] = new_total_cost\n",
    "\t\t\t\t\tchild_node = self.node_class(\n",
    "\t\t\t\t\t\tstate=child_state, \n",
    "\t\t\t\t\t\tparent=current_node, \n",
    "\t\t\t\t\t\taction=action, \n",
    "\t\t\t\t\t\tg_cost=new_g_cost, \n",
    "\t\t\t\t\t\th_cost=h_cost,\n",
    "\t\t\t\t\t\tdepth=current_node.depth+1\n",
    "\t\t\t\t\t)\n",
    "\n",
    "\t\t\t\t\theapq.heappush(self.queue, child_node)\n",
    "\n",
    "\t\treturn None, steps, time.time()-start_time\n",
    "\n",
    "num_runs = 1\n",
    "score_sorting = False\n",
    "num_buffers = 4\n",
    "time_limit = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------StrapGA--------\n",
      "Time limit exceeded\n",
      "plan: [31538, 4758, 48035, 18070, 21686, 17591]\n",
      "elapsed_time: 20.005s\n",
      "steps: 5594\n",
      "Moved 3 to: [15 18] | cost: 0.717 | done: False\n",
      "Moved 0 to: [47 38] | cost: 1.009 | done: False\n",
      "Moved 4 to: [80 15] | cost: 0.938 | done: False\n",
      "Moved 1 to: [80 50] | cost: 1.407 | done: False\n",
      "Moved 2 to: [16 66] | cost: 1.177 | done: False\n",
      "Moved 1 to: [75 71] | cost: 1.079 | done: True\n",
      "episode cost: 6.328\n",
      "         1234719 function calls (1230584 primitive calls) in 20.080 seconds\n",
      "\n",
      "   Ordered by: internal time\n",
      "   List reduced from 214 to 30 due to restriction <30>\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "     4216    2.460    0.001    4.327    0.001 d:\\Users\\arman\\Documents\\Uni\\Master\\Taarlab\\Rearrangement-Planning\\core\\env\\scene_manager.py:1099(valid_center_mask)\n",
      "    15191    1.473    0.000    2.023    0.000 d:\\Users\\arman\\Documents\\Uni\\Master\\Taarlab\\Rearrangement-Planning\\core\\env\\scene_manager.py:1157(is_invalid_center)\n",
      "    55064    1.160    0.000    1.160    0.000 {method 'clone' of 'torch._C.TensorBase' objects}\n",
      "     4123    0.803    0.000    1.415    0.000 d:\\Users\\arman\\Documents\\Uni\\Master\\Taarlab\\Rearrangement-Planning\\core\\env\\scene_manager.py:1512(find_blocking_objects)\n",
      "     6159    0.667    0.000    1.546    0.000 d:\\Users\\arman\\Documents\\Uni\\Master\\Taarlab\\Rearrangement-Planning\\core\\env\\scene_manager.py:1307(move_obj_with_stacked_ones)\n",
      "    16546    0.621    0.000    0.621    0.000 {method 'nonzero' of 'torch._C.TensorBase' objects}\n",
      "    57615    0.609    0.000    0.609    0.000 C:\\Users\\arman\\AppData\\Local\\Temp\\ipykernel_16552\\1996717269.py:34(ucb)\n",
      "     4909    0.566    0.000    0.775    0.000 d:\\Users\\arman\\Documents\\Uni\\Master\\Taarlab\\Rearrangement-Planning\\core\\env\\scene_manager.py:966(encode_move)\n",
      "     4602    0.504    0.000    0.905    0.000 C:\\Users\\arman\\AppData\\Local\\Temp\\ipykernel_16552\\1996717269.py:189(get_remaining_objs)\n",
      "     4123    0.477    0.000    0.477    0.000 {built-in method torch._unique2}\n",
      "     6159    0.463    0.000    0.540    0.000 d:\\Users\\arman\\Documents\\Uni\\Master\\Taarlab\\Rearrangement-Planning\\core\\env\\scene_manager.py:915(_draw_to_table)\n",
      "     6159    0.447    0.000    0.513    0.000 d:\\Users\\arman\\Documents\\Uni\\Master\\Taarlab\\Rearrangement-Planning\\core\\env\\scene_manager.py:905(_erase_from_table)\n",
      "    11890    0.425    0.000    0.425    0.000 {built-in method torch.tensor}\n",
      "     4216    0.394    0.000    5.172    0.001 d:\\Users\\arman\\Documents\\Uni\\Master\\Taarlab\\Rearrangement-Planning\\core\\env\\scene_manager.py:1452(get_empty_positions)\n",
      "    12472    0.372    0.000    1.366    0.000 d:\\Users\\arman\\Documents\\Uni\\Master\\Taarlab\\Rearrangement-Planning\\core\\env\\scene_manager.py:1250(cal_manipulator_movement)\n",
      "     6159    0.351    0.000    0.661    0.000 d:\\Users\\arman\\Documents\\Uni\\Master\\Taarlab\\Rearrangement-Planning\\core\\env\\scene_manager.py:93(build_child_of)\n",
      "     9987    0.349    0.000    0.349    0.000 {built-in method torch.nonzero}\n",
      "     8432    0.329    0.000    0.329    0.000 {method 'cumsum' of 'torch._C.TensorBase' objects}\n",
      "     9911    0.307    0.000    0.307    0.000 {built-in method torch.randperm}\n",
      "    15203    0.306    0.000    0.306    0.000 {method 'any' of 'torch._C.TensorBase' objects}\n",
      "    12472    0.302    0.000    0.302    0.000 {built-in method torch._C._linalg.linalg_vector_norm}\n",
      "     4517    0.296    0.000    8.572    0.002 C:\\Users\\arman\\AppData\\Local\\Temp\\ipykernel_16552\\1996717269.py:241(get_motion)\n",
      "    19081    0.294    0.000    0.294    0.000 {method 'float' of 'torch._C.TensorBase' objects}\n",
      "    12472    0.279    0.000    0.591    0.000 d:\\Users\\arman\\Documents\\Uni\\Master\\Taarlab\\Rearrangement-Planning\\venv\\lib\\site-packages\\torch\\functional.py:1501(norm)\n",
      "     6159    0.272    0.000    0.586    0.000 d:\\Users\\arman\\Documents\\Uni\\Master\\Taarlab\\Rearrangement-Planning\\core\\env\\scene_manager.py:40(get_object_below)\n",
      "     4441    0.244    0.000    0.626    0.000 C:\\Users\\arman\\AppData\\Local\\Temp\\ipykernel_16552\\1996717269.py:212(evaluate_state)\n",
      "     6159    0.230    0.000    5.639    0.001 d:\\Users\\arman\\Documents\\Uni\\Master\\Taarlab\\Rearrangement-Planning\\core\\env\\scene_manager.py:1328(move_func)\n",
      "     4216    0.226    0.000    0.226    0.000 {built-in method torch._C._nn.pad}\n",
      "    10212    0.224    0.000    0.225    0.000 {method 'all' of 'torch._C.TensorBase' objects}\n",
      "     4123    0.221    0.000    6.127    0.001 C:\\Users\\arman\\AppData\\Local\\Temp\\ipykernel_16552\\1996717269.py:224(get_action_move_obj_away)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class Strap(Astar):\n",
    "\tdef get_remaining_objs(self, state: Dict[str, torch.Tensor]) -> List[int]:\n",
    "\t\t\"\"\"\n",
    "\t\tObjects whose current center ≠ target center.\n",
    "\t\t\"\"\"\n",
    "\t\tcurrent_x, target_x = state['current'], self.env.target_x\n",
    "\n",
    "\t\t# All current vs. target centers: [N,2]\n",
    "\t\tcur_centers = current_x[:, Indices.COORD]\n",
    "\t\ttgt_centers = target_x[:, Indices.COORD]\n",
    "\n",
    "\t\t# Base condition for objs that should be on the table\n",
    "\t\tsatisfied = (cur_centers == tgt_centers).all(dim=1)  # [N]\n",
    "\n",
    "\t\t# Remaining = those not satisfied\n",
    "\t\trem = torch.nonzero(~satisfied, as_tuple=False).view(-1)  # [R]\n",
    "\n",
    "\t\tif rem.numel() == 0:\n",
    "\t\t\treturn []\n",
    "\n",
    "\t\t# Shuffle the remaining indices\n",
    "\t\tperm = torch.randperm(rem.size(0))\n",
    "\t\treturn rem[perm].tolist()\n",
    "\n",
    "\tdef get_valid_actions(self, state: Dict[str, torch.Tensor]) -> List[int]:\n",
    "\t\t\"\"\"\n",
    "\t\tFor each object k that’s not yet in place,\n",
    "\t\tgather up to self.num_buffers candidate coords\n",
    "\t\t(including its target if free), and batch‐encode\n",
    "\t\tall those 'move' actions at once.\n",
    "\t\t\"\"\"\n",
    "\t\t# restore the env to this state\n",
    "\t\tself.env.set_state(copy_state(state))\n",
    "\n",
    "\t\t# which objects remain to be placed?\n",
    "\t\trem = self.get_remaining_objs(state)\n",
    "\n",
    "\t\t# for each remaining object, batch‐fetch positions & encode\n",
    "\t\tvalid_actions = []\n",
    "\t\tfor k in rem:\n",
    "\t\t\t# coords: Tensor of shape [M,2], dtype long\n",
    "\t\t\tcoords = self.env.get_empty_positions_with_target(\n",
    "\t\t\t\tref_obj=k,\n",
    "\t\t\t\tn=self.num_buffers,\n",
    "\t\t\t\tsort=self.score_sorting\n",
    "\t\t\t)\n",
    "\n",
    "\t\t\tif coords.numel() == 0:\n",
    "\t\t\t\tcontinue\n",
    "\n",
    "\t\t\t# build batched start/target vectors of length M\n",
    "\t\t\tM      = coords.size(0)\n",
    "\t\t\tstarts = torch.full((M,), k, dtype=torch.long)\n",
    "\n",
    "\t\t\t# vectorized call: returns LongTensor[M]\n",
    "\t\t\tcodes  = self.env.encode_move(starts, coords)\n",
    "\t\t\tvalid_actions.append(codes)\n",
    "\n",
    "\t\tif len(valid_actions) == 0:\n",
    "\t\t\treturn []\n",
    "\n",
    "\t\t# concatenate all batches and return Python ints\n",
    "\t\treturn torch.cat(valid_actions).tolist()\n",
    "\n",
    "\tdef evaluate_state(self, state: Dict[str, torch.Tensor]) -> float:\n",
    "\t\t\"\"\"\n",
    "\t\tHeuristic = sum over all remaining k of:\n",
    "\t\tpp_cost + normalized_distance( current_pos[k], target_pos[k] ).\n",
    "\t\t\"\"\"\n",
    "\t\tcurrent_x, target_x = state['current'], self.env.target_x\n",
    "\n",
    "\t\t# 1) remaining object indices [R]\n",
    "\t\trem_nodes = torch.tensor(self.get_remaining_objs(state), dtype=torch.long)\n",
    "\t\tif rem_nodes.numel() == 0:\n",
    "\t\t\treturn 0.0\n",
    "\n",
    "\t\t# 2) Gather current and target centers for those nodes: [R,2]\n",
    "\t\tcur_ctr = current_x[rem_nodes, Indices.COORD].float()\n",
    "\t\ttgt_ctr = target_x[rem_nodes, Indices.COORD].float()\n",
    "\n",
    "\t\t# 3) Euclidean distances [R]\n",
    "\t\tdists   = torch.cdist(cur_ctr, tgt_ctr, p=2).diag()  # [R]\n",
    "\t\t\n",
    "\t\t# 4) Heuristic = R * pp_cost + sum(dists) * normalization\n",
    "\t\tR       = float(rem_nodes.size(0))\n",
    "\t\tpp      = env.pp_cost\n",
    "\t\tnorm    = env.normalization_factor\n",
    "\n",
    "\t\treturn R * pp + (dists.sum().item() * norm)\n",
    "\n",
    "\tdef solve(self, num_buffers: int=3, score_sorting: bool=False, time_limit: int=1000):\n",
    "\t\tif torch.sum(self.env.initial_x[:, Indices.RELATION]) > 0:\n",
    "\t\t\traise ValueError('Initial scene has stacks in Non-stack mode')\n",
    "\t\tif torch.sum(self.env.current_x[:, Indices.RELATION]) > 0:\n",
    "\t\t\traise ValueError('Current scene has stacks in Non-stack mode')\n",
    "\t\tif torch.sum(self.env.target_x[:, Indices.RELATION]) > 0:\n",
    "\t\t\traise ValueError('Target scene has stacks in Non-stack mode')\n",
    "\t\t\n",
    "\t\tself.score_sorting = score_sorting\n",
    "\t\tself.num_buffers = num_buffers\n",
    "\t\treturn self._solve(time_limit)\n",
    "\n",
    "class StrapGA(Strap):\n",
    "\tdef goal_attempt(self, node, time_limit: int) -> int:\n",
    "\t\tself.env.set_state(node.get_state())\n",
    "\t\tplan_to_go, steps, _ = Labbe(self.env).solve(time_limit=time_limit)\n",
    "\n",
    "\t\t# no feasible plan was found in the time_limit\n",
    "\t\tif plan_to_go is None:\n",
    "\t\t\treturn steps\n",
    "\n",
    "\t\t# --- Stage 1: Immediate Redundancy Removal ---\n",
    "\t\tdecoded_plan = []\n",
    "\t\tfor action in plan_to_go:\n",
    "\t\t\tdecoded_action = (action, self.env.decode_action(action))\n",
    "\t\t\t# each entry: (action, (action_type, start_obj, target_obj, coord))\n",
    "\t\t\t# remove redundant action on the latest manipulated object\n",
    "\t\t\t# This check is for consecutive actions on the SAME object.\n",
    "\t\t\t# It keeps the last action on that object and discards previous consecutive ones.\n",
    "\t\t\tif len(decoded_plan) > 0 and decoded_action[1][1] == decoded_plan[-1][1][1]:\n",
    "\t\t\t\tdecoded_plan[-1] = decoded_action\n",
    "\t\t\telse:\n",
    "\t\t\t\tdecoded_plan.append(decoded_action)\n",
    "\n",
    "\t\t# --- Stage 2: State-Checking Redundancy (Simulate and Filter) ---\n",
    "\t\t# This stage is only required after the immediate redundancy removal\n",
    "\t\tself.env.set_state(node.get_state())\n",
    "\t\tfeasible_path_cost = node.g_cost\n",
    "\t\trefined_plan = []\n",
    "\n",
    "\t\tfor i, decoded_action in enumerate(decoded_plan):\n",
    "\t\t\taction = decoded_action[0]\n",
    "\t\t\taction_type, start_obj, target_obj, coordinates = decoded_action[1]\n",
    "\t\t\t\n",
    "\t\t\tif action_type == 'stack':\n",
    "\t\t\t\t# If the object is already stacked, skip this action\n",
    "\t\t\t\tif self.env.current_x[start_obj, Indices.RELATION.start + target_obj] == 1:\n",
    "\t\t\t\t\tcontinue\n",
    "\t\t\telif action_type == 'move':\n",
    "\t\t\t\t# If the object is ALREADY at the target coordinates for this move action.\n",
    "\t\t\t\tcurrent_coord = self.env.current_x[start_obj, Indices.COORD]\n",
    "\t\t\t\tif torch.equal(current_coord, coordinates):\n",
    "\t\t\t\t\tcontinue\n",
    "\n",
    "\t\t\trefined_plan.append(action)\n",
    "\t\t\tcost, child_state = self.env._step(action_type, start_obj, target_obj, coordinates)\n",
    "\n",
    "\t\t\tfeasible_path_cost += cost\n",
    "\t\t\tif i == 0:\n",
    "\t\t\t\tfirst_child = child_state\n",
    "\t\t\t\tfirst_action = action\n",
    "\t\t\t\tfirst_cost = feasible_path_cost\n",
    "\n",
    "\t\t# --- Stage 3: add the first child node if the plan is the best so far ---\n",
    "\t\tif feasible_path_cost < self.best_cost:\n",
    "\t\t\tself.best_plan = reconstruct_path(node) + list(refined_plan)\n",
    "\t\t\tself.best_cost = feasible_path_cost\n",
    "\t\t\tchild_node = self.node_class(\n",
    "\t\t\t\tstate=first_child,\n",
    "\t\t\t\tparent=node,\n",
    "\t\t\t\taction=first_action,\n",
    "\t\t\t\tg_cost=first_cost,\n",
    "\t\t\t\th_cost=self.evaluate_state(first_child),\n",
    "\t\t\t\tdepth=node.depth+1\n",
    "\t\t\t)\n",
    "\n",
    "\t\t\theapq.heappush(self.queue, child_node)\n",
    "\n",
    "\t\t# Remove all the nodes with their total cost is greater than the feasible path cost\n",
    "\t\t# for node in self.queue:\n",
    "\t\t# \tif node.total_cost > feasible_path_cost:\n",
    "\t\t# \t\tself.queue.remove(node)\n",
    "\n",
    "\t\treturn steps\n",
    "\n",
    "\tdef _solve(self, time_limit: int=1000):\n",
    "\t\tstart_time = time.time()\n",
    "\t\tself.best_plan = None\n",
    "\t\tself.best_cost = float('inf')\n",
    "\n",
    "\t\tsteps = 0\n",
    "\t\troot_state = self.env.get_state()\n",
    "\t\troot_node = self.node_class(\n",
    "\t\t\tstate=root_state, \n",
    "\t\t\tg_cost=0, \n",
    "\t\t\th_cost=self.evaluate_state(root_state)\n",
    "\t\t)\n",
    "\n",
    "\t\tself.queue = []\n",
    "\t\theapq.heappush(self.queue, root_node)\n",
    "\t\tvisited = {}\n",
    "\n",
    "\t\twhile self.queue:\n",
    "\t\t\tcurrent_node = heapq.heappop(self.queue)\n",
    "\n",
    "\t\t\t# Check if the current node's state matches the target state\n",
    "\t\t\tif self.env.is_terminal_state(current_node.state):\n",
    "\t\t\t\tif current_node.total_cost < self.best_cost:\n",
    "\t\t\t\t\treturn reconstruct_path(current_node), steps, time.time()-start_time\n",
    "\t\t\t\treturn self.best_plan, steps, time.time()-start_time\n",
    "\n",
    "\t\t\t# Check if the elapsed time has exceeded the limit\n",
    "\t\t\tif time.time()-start_time > time_limit:\n",
    "\t\t\t\tprint('Time limit exceeded')\n",
    "\t\t\t\treturn self.best_plan, steps, time.time()-start_time\n",
    "\n",
    "\t\t\tlast_obj = self.env.decode_action(current_node.action)[1] if current_node.action is not None else None\n",
    "\t\t\tfor action in self.get_valid_actions(current_node.state):\n",
    "\t\t\t\taction_type, start_obj, target_obj, coordinates = self.env.decode_action(action)\n",
    "\n",
    "\t\t\t\t# If the last changed obj is the same as the current obj, continue\n",
    "\t\t\t\tif start_obj == last_obj:\n",
    "\t\t\t\t\tcontinue\n",
    "\n",
    "\t\t\t\tsteps += 1\n",
    "\t\t\t\tself.env.set_state(current_node.get_state())\n",
    "\t\t\t\tcost, child_state = self.env._step(action_type, start_obj, target_obj, coordinates)\n",
    "\n",
    "\t\t\t\t# If state hasn't changed, continue\n",
    "\t\t\t\tif torch.equal(child_state['current'], current_node.state['current']):\n",
    "\t\t\t\t\traise ValueError('State has not changed')\n",
    "\n",
    "\t\t\t\tchild_hash = state_to_hashable(child_state)\n",
    "\n",
    "\t\t\t\t# Calculate the accumulated cost for the current path\n",
    "\t\t\t\tnew_g_cost = current_node.g_cost + cost\n",
    "\t\t\t\th_cost = self.evaluate_state(child_state)\n",
    "\t\t\t\tnew_total_cost = new_g_cost + h_cost\n",
    "\n",
    "\t\t\t\t# Retain the node with better cost\n",
    "\t\t\t\tif child_hash not in visited or visited[child_hash] > new_total_cost:\n",
    "\t\t\t\t\tvisited[child_hash] = new_total_cost\n",
    "\t\t\t\t\tchild_node = self.node_class(\n",
    "\t\t\t\t\t\tstate=child_state, \n",
    "\t\t\t\t\t\tparent=current_node, \n",
    "\t\t\t\t\t\taction=action, \n",
    "\t\t\t\t\t\tg_cost=new_g_cost, \n",
    "\t\t\t\t\t\th_cost=h_cost,\n",
    "\t\t\t\t\t\tdepth=current_node.depth+1\n",
    "\t\t\t\t\t)\n",
    "\n",
    "\t\t\t\t\theapq.heappush(self.queue, child_node)\n",
    "\n",
    "\t\t\tif time.time()-start_time > time_limit:\n",
    "\t\t\t\tprint('Time limit exceeded')\n",
    "\t\t\t\treturn self.best_plan, steps, time.time()-start_time\n",
    "\n",
    "\t\t\t# Goal Attempting\n",
    "\t\t\tsim_time_limit = (time_limit - time.time() + start_time) / 5\n",
    "\t\t\tsteps += self.goal_attempt(current_node, sim_time_limit)\n",
    "\n",
    "\t\treturn self.best_plan, steps, time.time()-start_time\n",
    "\n",
    "prof = cProfile.Profile()\n",
    "prof.enable()\n",
    "plan = evaluate_alg(\n",
    "\tenv, StrapGA, initial_scene, target_scene, \n",
    "\tnum_runs=num_runs, score_sorting=score_sorting,\n",
    "\tnum_buffers=num_buffers, time_limit=time_limit\n",
    ");\n",
    "prof.disable()\n",
    "pstats.Stats(prof).sort_stats('tottime').print_stats(30);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------StrapGA_S--------\n",
      "Time limit exceeded\n",
      "plan: [31538, 4758, 48035, 17869, 21686, 17591]\n",
      "elapsed_time: 20.038s\n",
      "steps: 4836\n",
      "Moved 3 to: [15 18] | cost: 0.717 | done: False\n",
      "Moved 0 to: [47 38] | cost: 1.009 | done: False\n",
      "Moved 4 to: [80 15] | cost: 0.938 | done: False\n",
      "Moved 1 to: [78 49] | cost: 1.393 | done: False\n",
      "Moved 2 to: [16 66] | cost: 1.186 | done: False\n",
      "Moved 1 to: [75 71] | cost: 1.068 | done: True\n",
      "episode cost: 6.311\n",
      "         1097295 function calls (1094409 primitive calls) in 20.132 seconds\n",
      "\n",
      "   Ordered by: internal time\n",
      "   List reduced from 223 to 30 due to restriction <30>\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "     2584    1.467    0.001    2.613    0.001 d:\\Users\\arman\\Documents\\Uni\\Master\\Taarlab\\Rearrangement-Planning\\core\\env\\scene_manager.py:1099(valid_center_mask)\n",
      "    39185    1.073    0.000    1.073    0.000 {method 'nonzero' of 'torch._C.TensorBase' objects}\n",
      "    10297    0.978    0.000    1.385    0.000 d:\\Users\\arman\\Documents\\Uni\\Master\\Taarlab\\Rearrangement-Planning\\core\\env\\scene_manager.py:1157(is_invalid_center)\n",
      "    49215    0.883    0.000    0.883    0.000 {method 'clone' of 'torch._C.TensorBase' objects}\n",
      "     3270    0.727    0.000    1.351    0.000 C:\\Users\\arman\\AppData\\Local\\Temp\\ipykernel_16552\\1996717269.py:275(get_remaining_objs)\n",
      "    11851    0.629    0.000    1.172    0.000 d:\\Users\\arman\\Documents\\Uni\\Master\\Taarlab\\Rearrangement-Planning\\core\\env\\scene_manager.py:50(get_object_above)\n",
      "     5470    0.605    0.000    1.427    0.000 d:\\Users\\arman\\Documents\\Uni\\Master\\Taarlab\\Rearrangement-Planning\\core\\env\\scene_manager.py:1307(move_obj_with_stacked_ones)\n",
      "     1566    0.539    0.000    1.842    0.001 C:\\Users\\arman\\AppData\\Local\\Temp\\ipykernel_16552\\1941049616.py:129(evaluate_state)\n",
      "     2533    0.498    0.000    0.884    0.000 d:\\Users\\arman\\Documents\\Uni\\Master\\Taarlab\\Rearrangement-Planning\\core\\env\\scene_manager.py:1512(find_blocking_objects)\n",
      "    11155    0.420    0.000    0.732    0.000 d:\\Users\\arman\\Documents\\Uni\\Master\\Taarlab\\Rearrangement-Planning\\core\\env\\scene_manager.py:40(get_object_below)\n",
      "     3039    0.419    0.000    0.991    0.000 C:\\Users\\arman\\AppData\\Local\\Temp\\ipykernel_16552\\1996717269.py:307(evaluate_state)\n",
      "     8113    0.412    0.000    0.817    0.000 d:\\Users\\arman\\Documents\\Uni\\Master\\Taarlab\\Rearrangement-Planning\\core\\env\\scene_manager.py:78(build_parent_of)\n",
      "    15177    0.411    0.000    0.411    0.000 {method 'any' of 'torch._C.TensorBase' objects}\n",
      "    10594    0.400    0.000    0.400    0.000 {built-in method torch.tensor}\n",
      "     1685    0.388    0.000    0.732    0.000 C:\\Users\\arman\\AppData\\Local\\Temp\\ipykernel_16552\\1941049616.py:2(get_remaining_objs)\n",
      "    30909    0.371    0.000    0.371    0.000 C:\\Users\\arman\\AppData\\Local\\Temp\\ipykernel_16552\\1996717269.py:34(ucb)\n",
      "     4837    0.351    0.000    0.405    0.000 d:\\Users\\arman\\Documents\\Uni\\Master\\Taarlab\\Rearrangement-Planning\\core\\env\\scene_manager.py:905(_erase_from_table)\n",
      "     2941    0.347    0.000    0.452    0.000 d:\\Users\\arman\\Documents\\Uni\\Master\\Taarlab\\Rearrangement-Planning\\core\\env\\scene_manager.py:966(encode_move)\n",
      "     5470    0.344    0.000    0.625    0.000 d:\\Users\\arman\\Documents\\Uni\\Master\\Taarlab\\Rearrangement-Planning\\core\\env\\scene_manager.py:93(build_child_of)\n",
      "     3126    0.342    0.000    0.342    0.000 {built-in method torch.cat}\n",
      "    18339    0.340    0.000    0.341    0.000 {built-in method torch.full}\n",
      "    11168    0.339    0.000    1.216    0.000 d:\\Users\\arman\\Documents\\Uni\\Master\\Taarlab\\Rearrangement-Planning\\core\\env\\scene_manager.py:1250(cal_manipulator_movement)\n",
      "     2533    0.301    0.000    0.301    0.000 {built-in method torch._unique2}\n",
      "    18905    0.298    0.000    0.298    0.000 {method 'float' of 'torch._C.TensorBase' objects}\n",
      "     4082    0.297    0.000    0.353    0.000 d:\\Users\\arman\\Documents\\Uni\\Master\\Taarlab\\Rearrangement-Planning\\core\\env\\scene_manager.py:915(_draw_to_table)\n",
      "    10237    0.282    0.000    0.283    0.000 {built-in method torch.nonzero}\n",
      "     9166    0.271    0.000    0.271    0.000 {built-in method torch.randperm}\n",
      "    11168    0.268    0.000    0.268    0.000 {built-in method torch._C._linalg.linalg_vector_norm}\n",
      "    11168    0.256    0.000    0.533    0.000 d:\\Users\\arman\\Documents\\Uni\\Master\\Taarlab\\Rearrangement-Planning\\venv\\lib\\site-packages\\torch\\functional.py:1501(norm)\n",
      "     2698    0.252    0.000    0.491    0.000 d:\\Users\\arman\\Documents\\Uni\\Master\\Taarlab\\Rearrangement-Planning\\core\\env\\scene_manager.py:1420(get_empty_objs)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class Strap_S(Astar):\n",
    "\tdef get_remaining_objs(self, state: Dict[str, torch.Tensor]) -> List[int]:\n",
    "\t\t\"\"\"\n",
    "\t\t• Objects whose aren't stacked on their target objects.\n",
    "\t\t• Base objects whose current center ≠ target center.\n",
    "\t\t\"\"\"\n",
    "\t\tcurrent_x, target_x = state['current'], self.env.target_x\n",
    "\n",
    "\t\t# Build parent‐of maps once\n",
    "\t\tcur_parent = build_parent_of(current_x)\n",
    "\n",
    "\t\t# Stacking condition for objs that should be stacked\n",
    "\t\tcond_stacked = (self.tgt_parent >= 0) & (cur_parent == self.tgt_parent)\n",
    "\n",
    "\t\t# Base condition for objs that should be on the table\n",
    "\t\tcur_centers = current_x[:, Indices.COORD]\n",
    "\t\ttgt_centers = target_x[:, Indices.COORD]\n",
    "\t\tbase_match = (cur_centers == tgt_centers).all(dim=1)  # [N]\n",
    "\t\tcond_base    = (self.tgt_parent < 0) & (cur_parent < 0) & base_match\n",
    "\n",
    "\t\t# Satisfied = either stacking OK or base OK\n",
    "\t\tsatisfied = cond_stacked | cond_base       # [N]\n",
    "\n",
    "\t\t# Remaining = those not satisfied\n",
    "\t\trem = torch.nonzero(~satisfied, as_tuple=False).view(-1)  # [R]\n",
    "\n",
    "\t\tif rem.numel() == 0:\n",
    "\t\t\treturn []\n",
    "\n",
    "\t\t# Shuffle the remaining indices\n",
    "\t\tperm = torch.randperm(rem.size(0))\n",
    "\t\treturn rem[perm].tolist()\n",
    "\n",
    "\tdef get_valid_actions(self, state: Dict[str, torch.Tensor]) -> List[int]:\n",
    "\t\t\"\"\"\n",
    "\t\t...\n",
    "\t\t\"\"\"\n",
    "\t\t# restore the env to this state\n",
    "\t\tself.env.set_state(copy_state(state))\n",
    "\n",
    "\t\t# which objects remain to be placed?\n",
    "\t\trem = self.get_remaining_objs(state)\n",
    "\n",
    "\t\t# Which k are allowed (static_stack skips non‐empty actors)\n",
    "\t\tif self.static_stack:\n",
    "\t\t\trem = torch.tensor(rem, dtype=torch.long)\n",
    "\t\t\trel     = self.env.current_x[:, Indices.RELATION]\n",
    "\t\t\tempty_k = ~rel.any(dim=0)                  # True if k has no one on top\n",
    "\t\t\tmask = empty_k[rem]\n",
    "\t\t\tks = rem[mask].tolist()\n",
    "\t\telse:\n",
    "\t\t\tks = rem\n",
    "\n",
    "\t\tvalid_actions = []\n",
    "\t\tstack_nums = max(int(0.6 * self.num_buffers), 1)\n",
    "\n",
    "\t\tfor k in ks:\n",
    "\t\t\tvalid_stacks = []\n",
    "\t\t\tempty_objs = self.env.get_empty_objs(ref_obj=k, n=stack_nums)\n",
    "\t\t\tif len(empty_objs) > 0:\n",
    "\t\t\t\tM      = len(empty_objs)\n",
    "\t\t\t\tstarts = torch.full((M,), k, dtype=torch.long)\n",
    "\t\t\t\ttargets= torch.tensor(empty_objs, dtype=torch.long)\n",
    "\t\t\t\tvalid_stacks = self.env.encode_stack(starts, targets).tolist()\n",
    "\n",
    "\t\t\tvalid_moves = []\n",
    "\t\t\tcoords = self.env.get_empty_positions_with_target(\n",
    "\t\t\t\tref_obj=k,\n",
    "\t\t\t\tn=self.num_buffers-len(valid_stacks),\n",
    "\t\t\t\tsort=self.score_sorting\n",
    "\t\t\t)\n",
    "\n",
    "\t\t\tif coords.numel() > 0:\n",
    "\t\t\t\tM      = coords.size(0)\n",
    "\t\t\t\tstarts = torch.full((M,), k, dtype=torch.long)\n",
    "\t\t\t\tvalid_moves  = self.env.encode_move(starts, coords).tolist()\n",
    "\n",
    "\t\t\tvalid_actions += valid_stacks + valid_moves\n",
    "\n",
    "\t\treturn valid_actions\n",
    "\n",
    "\tdef evaluate_state2(self, state: Dict[str, torch.Tensor]) -> float:\n",
    "\t\t\"\"\"\n",
    "\t\tHeuristic for each remaining object k:\n",
    "\t\t\tAₖ = dist(Cₖ → Tₖ) * NF\n",
    "\t\t\tBₖ = min_j [ dist(Cₖ → Cⱼ) + dist(Tⱼ → Tₖ) ] * NF + pp_cost\n",
    "\t\t\t\t(only over supporting j)\n",
    "\t\tFinal h = sum_k ( min(Aₖ, Bₖ) + pp_cost )\n",
    "\t\t\"\"\"\n",
    "\t\tcur_x, tgt_x  = state['current'], self.env.target_x\n",
    "\t\tpp_cost, norm = self.env.pp_cost, self.env.normalization_factor\n",
    "\n",
    "\t\t# Remaining indices\n",
    "\t\trem = torch.tensor(self.get_remaining_objs(state), dtype=torch.long)\n",
    "\t\tif rem.numel() == 0:\n",
    "\t\t\treturn 0.0\n",
    "\n",
    "\t\t# Gather current & target centers for remaining: [R,2]\n",
    "\t\tcur_ctr = cur_x[rem, Indices.COORD].float()  # [R,2]\n",
    "\t\ttgt_ctr = tgt_x[rem, Indices.COORD].float()  # [R,2]\n",
    "\n",
    "\t\t# Compute A = direct move distance\n",
    "\t\tA = torch.cdist(cur_ctr, tgt_ctr, p=2).diag() * norm     # [R]\n",
    "\n",
    "\t\t# Precompute all object centers\n",
    "\t\tall_cur = cur_x[:, Indices.COORD].float()    # [N,2]\n",
    "\t\tall_tgt = tgt_x[:, Indices.COORD].float()    # [N,2]\n",
    "\n",
    "\t\t# Pairwise distances\n",
    "\t\tD_c = torch.cdist(cur_ctr, all_cur, p=2)  # cost C_k→C_j: [R,N]\n",
    "\t\tD_t = torch.cdist(tgt_ctr, all_tgt, p=2)  # cost T_k→T_j: [R,N]\n",
    "\n",
    "\t\t# Stability mask for rem rows\n",
    "\t\tS_rem = self.env.stability_mask[rem]      # [R,N]\n",
    "\n",
    "\t\t# Compute stack‐move costs and invalidate unsupportable pairs\n",
    "\t\tcosts = D_c + D_t                   # [R,N]\n",
    "\t\tcosts[~S_rem] = float('inf')        # forbid non‐stable\n",
    "\n",
    "\t\t# B = min_j costs[r,j] * norm + pp_cost\n",
    "\t\tB = costs.min(dim=1).values * norm + pp_cost           # [R]\n",
    "\n",
    "\t\t# Per‐object best cost = min(A, B) + pp_cost\n",
    "\t\tbest = torch.minimum(A, B) + pp_cost                 # [R]\n",
    "\n",
    "\t\t# Final h_cost\n",
    "\t\treturn best.sum()\n",
    "\n",
    "\tdef evaluate_state(self, state: Dict[str, torch.Tensor]) -> float:\n",
    "\t\t\"\"\"\n",
    "\t\tHeuristic for each remaining object k:\n",
    "\t\t\tAₖ = dist(Cₖ → Tₖ) * NF\n",
    "\t\t\tBₖ = min_j [ dist(Cₖ → Cⱼ) ] * NF + pp_cost\n",
    "\t\t\t\t(only over supporting j)\n",
    "\t\tFinal h = sum_k ( min(Aₖ, Bₖ) + pp_cost )\n",
    "\t\t\"\"\"\n",
    "\t\tcur_x, tgt_x  = state['current'], self.env.target_x\n",
    "\t\tpp_cost, norm = self.env.pp_cost, self.env.normalization_factor\n",
    "\n",
    "\t\t# Remaining indices\n",
    "\t\trem = torch.tensor(self.get_remaining_objs(state), dtype=torch.long)\n",
    "\t\tif rem.numel() == 0:\n",
    "\t\t\treturn 0.0\n",
    "\n",
    "\t\t# Gather current & target centers for remaining: [R,2]\n",
    "\t\tcur_ctr = cur_x[rem, Indices.COORD].float()  # [R,2]\n",
    "\t\ttgt_ctr = tgt_x[rem, Indices.COORD].float()  # [R,2]\n",
    "\n",
    "\t\t# Compute A = direct move distance\n",
    "\t\tA = torch.cdist(cur_ctr, tgt_ctr, p=2).diag() * norm     # [R]\n",
    "\n",
    "\t\t# Precompute all object centers\n",
    "\t\tall_cur = cur_x[:, Indices.COORD].float()    # [N,2]\n",
    "\n",
    "\t\t# Pairwise distances\n",
    "\t\tcosts = torch.cdist(cur_ctr, all_cur, p=2)  # cost C_k→C_j: [R,N]\n",
    "\n",
    "\t\t# Stability mask for rem rows\n",
    "\t\tS_rem = self.env.stability_mask[rem]      # [R,N]\n",
    "\n",
    "\t\t# Compute stack‐move costs and invalidate unsupportable pairs\n",
    "\t\tcosts[~S_rem] = float('inf')        # forbid non‐stable\n",
    "\n",
    "\t\t# B = min_j costs[r,j] * norm + pp_cost\n",
    "\t\tB = costs.min(dim=1).values * norm + pp_cost           # [R]\n",
    "\n",
    "\t\t# Per‐object best cost = min(A, B) + pp_cost\n",
    "\t\tbest = torch.minimum(A, B) + pp_cost                 # [R]\n",
    "\n",
    "\t\t# Final h_cost\n",
    "\t\treturn best.sum()\n",
    "\n",
    "\tdef solve(self, num_buffers: int=4, score_sorting: bool=False, time_limit: int=1000, static_stack: bool=False):\n",
    "\t\tself.tgt_parent = build_parent_of(self.env.target_x)\n",
    "\t\tself.score_sorting = score_sorting\n",
    "\t\tself.num_buffers = num_buffers\n",
    "\t\tself.static_stack = static_stack\n",
    "\t\tself.env.static_stack = static_stack\n",
    "\t\treturn self._solve(time_limit)\n",
    "\n",
    "class StrapGA_S(Strap_S):\n",
    "\tdef goal_attempt(self, node, time_limit: int) -> int:\n",
    "\t\tself.env.set_state(node.get_state())\n",
    "\t\tplan_to_go, steps, _ = Labbe_S(self.env).solve(time_limit=time_limit, static_stack=self.static_stack)\n",
    "\n",
    "\t\t# no feasible plan was found in the time_limit\n",
    "\t\tif plan_to_go is None:\n",
    "\t\t\treturn steps\n",
    "\n",
    "\t\t# --- Stage 1: Immediate Redundancy Removal ---\n",
    "\t\tdecoded_plan = []\n",
    "\t\tfor action in plan_to_go:\n",
    "\t\t\tdecoded_action = (action, self.env.decode_action(action))\n",
    "\t\t\t# each entry: (action, (action_type, start_obj, target_obj, coord))\n",
    "\t\t\t# remove redundant action on the latest manipulated object\n",
    "\t\t\t# This check is for consecutive actions on the SAME object.\n",
    "\t\t\t# It keeps the last action on that object and discards previous consecutive ones.\n",
    "\t\t\tif len(decoded_plan) > 0 and decoded_action[1][1] == decoded_plan[-1][1][1]:\n",
    "\t\t\t\tdecoded_plan[-1] = decoded_action\n",
    "\t\t\telse:\n",
    "\t\t\t\tdecoded_plan.append(decoded_action)\n",
    "\n",
    "\t\t# --- Stage 2: State-Checking Redundancy (Simulate and Filter) ---\n",
    "\t\t# This stage is only required after the immediate redundancy removal\n",
    "\t\tself.env.set_state(node.get_state())\n",
    "\t\tfeasible_path_cost = node.g_cost\n",
    "\t\trefined_plan = []\n",
    "\n",
    "\t\tfor i, decoded_action in enumerate(decoded_plan):\n",
    "\t\t\taction = decoded_action[0]\n",
    "\t\t\taction_type, start_obj, target_obj, coordinates = decoded_action[1]\n",
    "\t\t\t\n",
    "\t\t\tif action_type == 'stack':\n",
    "\t\t\t\t# If the object is already stacked, skip this action\n",
    "\t\t\t\tif self.env.current_x[start_obj, Indices.RELATION.start + target_obj] == 1:\n",
    "\t\t\t\t\tcontinue\n",
    "\t\t\telif action_type == 'move':\n",
    "\t\t\t\t# If the object is ALREADY at the target coordinates for this move action.\n",
    "\t\t\t\tcurrent_coord = self.env.current_x[start_obj, Indices.COORD]\n",
    "\t\t\t\tif torch.equal(current_coord, coordinates):\n",
    "\t\t\t\t\tcontinue\n",
    "\n",
    "\t\t\trefined_plan.append(action)\n",
    "\t\t\tcost, child_state = self.env._step(action_type, start_obj, target_obj, coordinates)\n",
    "\n",
    "\t\t\tfeasible_path_cost += cost\n",
    "\t\t\tif i == 0:\n",
    "\t\t\t\tfirst_child = child_state\n",
    "\t\t\t\tfirst_action = action\n",
    "\t\t\t\tfirst_cost = feasible_path_cost\n",
    "\n",
    "\t\t# --- Stage 3: add the first child node if the plan is the best so far ---\n",
    "\t\tif feasible_path_cost < self.best_cost:\n",
    "\t\t\tself.best_plan = reconstruct_path(node) + list(refined_plan)\n",
    "\t\t\tself.best_cost = feasible_path_cost\n",
    "\t\t\tchild_node = self.node_class(\n",
    "\t\t\t\tstate=first_child,\n",
    "\t\t\t\tparent=node,\n",
    "\t\t\t\taction=first_action,\n",
    "\t\t\t\tg_cost=first_cost,\n",
    "\t\t\t\th_cost=self.evaluate_state(first_child),\n",
    "\t\t\t\tdepth=node.depth+1\n",
    "\t\t\t)\n",
    "\n",
    "\t\t\theapq.heappush(self.queue, child_node)\n",
    "\n",
    "\t\t# Remove all the nodes with their total cost is greater than the feasible path cost\n",
    "\t\t# for node in self.queue:\n",
    "\t\t# \tif node.total_cost > feasible_path_cost:\n",
    "\t\t# \t\tself.queue.remove(node)\n",
    "\n",
    "\t\treturn steps\n",
    "\n",
    "\tdef _solve(self, time_limit: int=1000):\n",
    "\t\tstart_time = time.time()\n",
    "\t\tself.best_plan = None\n",
    "\t\tself.best_cost = float('inf')\n",
    "\n",
    "\t\tsteps = 0\n",
    "\t\troot_state = self.env.get_state()\n",
    "\t\troot_node = self.node_class(\n",
    "\t\t\tstate=root_state, \n",
    "\t\t\tg_cost=0, \n",
    "\t\t\th_cost=self.evaluate_state(root_state)\n",
    "\t\t)\n",
    "\n",
    "\t\tself.queue = []\n",
    "\t\theapq.heappush(self.queue, root_node)\n",
    "\t\tvisited = {}\n",
    "\n",
    "\t\twhile self.queue:\n",
    "\t\t\tcurrent_node = heapq.heappop(self.queue)\n",
    "\n",
    "\t\t\t# Check if the current node's state matches the target state\n",
    "\t\t\tif self.env.is_terminal_state(current_node.state):\n",
    "\t\t\t\tif current_node.total_cost < self.best_cost:\n",
    "\t\t\t\t\treturn reconstruct_path(current_node), steps, time.time()-start_time\n",
    "\t\t\t\treturn self.best_plan, steps, time.time()-start_time\n",
    "\n",
    "\t\t\t# Check if the elapsed time has exceeded the limit\n",
    "\t\t\tif time.time()-start_time > time_limit:\n",
    "\t\t\t\tprint('Time limit exceeded')\n",
    "\t\t\t\treturn self.best_plan, steps, time.time()-start_time\n",
    "\n",
    "\t\t\tlast_obj = self.env.decode_action(current_node.action)[1] if current_node.action is not None else None\n",
    "\t\t\tfor action in self.get_valid_actions(current_node.state):\n",
    "\t\t\t\taction_type, start_obj, target_obj, coordinates = self.env.decode_action(action)\n",
    "\n",
    "\t\t\t\t# If the last changed obj is the same as the current obj, continue\n",
    "\t\t\t\tif start_obj == last_obj:\n",
    "\t\t\t\t\tcontinue\n",
    "\n",
    "\t\t\t\tsteps += 1\n",
    "\t\t\t\tself.env.set_state(current_node.get_state())\n",
    "\t\t\t\tcost, child_state = self.env._step(action_type, start_obj, target_obj, coordinates)\n",
    "\n",
    "\t\t\t\t# If state hasn't changed, continue\n",
    "\t\t\t\tif torch.equal(child_state['current'], current_node.state['current']):\n",
    "\t\t\t\t\traise ValueError('State has not changed')\n",
    "\n",
    "\t\t\t\tchild_hash = state_to_hashable(child_state)\n",
    "\n",
    "\t\t\t\t# Calculate the accumulated cost for the current path\n",
    "\t\t\t\tnew_g_cost = current_node.g_cost + cost\n",
    "\t\t\t\th_cost = self.evaluate_state(child_state)\n",
    "\t\t\t\tnew_total_cost = new_g_cost + h_cost\n",
    "\n",
    "\t\t\t\t# Retain the node with better cost\n",
    "\t\t\t\tif child_hash not in visited or visited[child_hash] > new_total_cost:\n",
    "\t\t\t\t\tvisited[child_hash] = new_total_cost\n",
    "\t\t\t\t\tchild_node = self.node_class(\n",
    "\t\t\t\t\t\tstate=child_state, \n",
    "\t\t\t\t\t\tparent=current_node, \n",
    "\t\t\t\t\t\taction=action, \n",
    "\t\t\t\t\t\tg_cost=new_g_cost, \n",
    "\t\t\t\t\t\th_cost=h_cost,\n",
    "\t\t\t\t\t\tdepth=current_node.depth+1\n",
    "\t\t\t\t\t)\n",
    "\n",
    "\t\t\t\t\theapq.heappush(self.queue, child_node)\n",
    "\n",
    "\t\t\tif time.time()-start_time > time_limit:\n",
    "\t\t\t\tprint('Time limit exceeded')\n",
    "\t\t\t\treturn self.best_plan, steps, time.time()-start_time\n",
    "\n",
    "\t\t\t# Goal Attempting\n",
    "\t\t\tsim_time_limit = (time_limit - time.time() + start_time) / 5\n",
    "\t\t\tsteps += self.goal_attempt(current_node, sim_time_limit)\n",
    "\n",
    "\t\treturn self.best_plan, steps, time.time()-start_time\n",
    "\n",
    "prof = cProfile.Profile()\n",
    "prof.enable()\n",
    "evaluate_alg(\n",
    "\tenv, StrapGA_S, initial_scene, target_scene, \n",
    "\tnum_runs=num_runs, score_sorting=score_sorting, static_stack=True,\n",
    "\tnum_buffers=num_buffers, time_limit=time_limit\n",
    ");\n",
    "prof.disable()\n",
    "pstats.Stats(prof).sort_stats('tottime').print_stats(30);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Refinement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def refine_until_convergence(env, plan, initial_scene, target_scene, alg, verbose=0):\n",
    "\tstart_time = time.time()\n",
    "\tbest_plan = plan\n",
    "\tbest_cost = env_cost(env, plan, initial_scene, target_scene, log=False)\n",
    "\tif plan is None:\n",
    "\t\treturn best_plan, best_cost, time.time() - start_time\n",
    "\twhile True:\n",
    "\t\tif alg in ['Labbe', 'Strap', 'StrapGA']:\n",
    "\t\t\trefined_plan = plan_refinement(env, plan, initial_scene, target_scene, verbose=verbose)\n",
    "\t\telse:\n",
    "\t\t\trefined_plan = plan_refinement_stack(env, plan, initial_scene, target_scene, verbose=verbose)\n",
    "\n",
    "\t\tif plan == refined_plan:\n",
    "\t\t\tbreak\n",
    "\n",
    "\t\tcost = env_cost(env, refined_plan, initial_scene, target_scene, log=False)\n",
    "\t\tif cost < best_cost:\n",
    "\t\t\tif verbose > 0:\n",
    "\t\t\t\tprint(f'cost got better from {best_cost:.3f} to {cost:.3f}')\n",
    "\t\t\tbest_cost = cost\n",
    "\t\t\tbest_plan = refined_plan\n",
    "\n",
    "\t\tplan = refined_plan\n",
    "\n",
    "\treturn best_plan, best_cost, time.time() - start_time\n",
    "\n",
    "def plan_refinement(env, plan, initial_scene, target_scene, verbose=0):\n",
    "\t# --- Stage 1: Immediate Redundancy Removal ---\n",
    "\tdecoded_plan = []\n",
    "\tfor action in plan:\n",
    "\t\tdecoded_action = (action, env.decode_action(action))\n",
    "\t\tif decoded_action[1][0] == 'stack':\n",
    "\t\t\tprint(\"There is a stack action in simple refinement\")\n",
    "\t\t\treturn plan\n",
    "\t\t# each entry: (action, (action_type, start_obj, target_obj, coord))\n",
    "\t\t# remove redundant action on the latest manipulated object\n",
    "\t\t# This check is for consecutive actions on the SAME object.\n",
    "\t\t# It keeps the last action on that object and discards previous consecutive ones.\n",
    "\t\tif len(decoded_plan) > 0 and decoded_action[1][1] == decoded_plan[-1][1][1]:\n",
    "\t\t\tif verbose > 0:\n",
    "\t\t\t\tprint(f'Redundant action, obj {decoded_plan[-1][1][1]} moving to {decoded_plan[-1][1][3]}, was removed')\n",
    "\t\t\tdecoded_plan[-1] = decoded_action\n",
    "\t\telse:\n",
    "\t\t\tdecoded_plan.append(decoded_action)\n",
    "\n",
    "\t# --- Stage 2: State-Checking Redundancy (Simulate and Filter) ---\n",
    "\t# This stage is only required after the immediate redundancy removal\n",
    "\tenv.reset(initial_scene, target_scene)\n",
    "\taction_seq = []\n",
    "\tplan = []\n",
    "\n",
    "\tfor decoded_action in decoded_plan:\n",
    "\t\taction = decoded_action[0]\n",
    "\t\taction_type, start_obj, target_obj, coordinates = decoded_action[1]\n",
    "\t\t\n",
    "\t\t# If the object is ALREADY at the target coordinates for this move action.\n",
    "\t\tcurrent_coord = env.current_x[start_obj, Indices.COORD]\n",
    "\t\tif torch.equal(current_coord, coordinates):\n",
    "\t\t\tif verbose > 0:\n",
    "\t\t\t\tprint(f\"Skipping Move action (obj {start_obj} to {coordinates.tolist()}): Already at target position.\")\n",
    "\t\t\tcontinue\n",
    "\n",
    "\t\tp_place = coordinates.clone()\n",
    "\t\tp_pick = env.current_x[start_obj, Indices.COORD].clone()\n",
    "\n",
    "\t\tplan.append(action)\n",
    "\t\taction_seq.append({\n",
    "\t\t\t'k': start_obj,\n",
    "\t\t\t'p_pick': p_pick,\n",
    "\t\t\t'p_place': p_place\n",
    "\t\t})\n",
    "\n",
    "\t\tenv._step(action_type, start_obj, target_obj, coordinates)\n",
    "\n",
    "\tenv.reset(initial_scene, target_scene)\n",
    "\tB = {}\n",
    "\tH = {0: env.get_state()}\t\t# arrangement history\n",
    "\tfor i in range(len(action_seq)):\n",
    "\t\tk = action_seq[i]['k']\n",
    "\n",
    "\t\tif k in B:\t\t\t# if object k was moved\n",
    "\t\t\tbIdx = B[k]\t\t# previous action index on k\n",
    "\n",
    "\t\t\t# Valid mask form the action index bIdx to i-1\n",
    "\t\t\tvalid_mask = torch.ones(env.grid_size, dtype=torch.bool)\n",
    "\t\t\tfor j in range(bIdx-1, i):\n",
    "\t\t\t\tenv.set_state(copy_state(H[j+1]))\n",
    "\t\t\t\tvalid_mask &= env.valid_center_mask(k)\n",
    "\n",
    "\t\t\tenv.set_state(copy_state(H[i]))\n",
    "\n",
    "\t\t\tP = valid_mask.nonzero(as_tuple=False).float()\n",
    "\t\t\tif len(P) == 0:\n",
    "\t\t\t\tif verbose > 0:\n",
    "\t\t\t\t\tprint(f'No feasible buffer set')\n",
    "\n",
    "\t\t\t# Distances\n",
    "\t\t\tp1 = action_seq[bIdx]['p_pick'].float()\n",
    "\t\t\tp2 = action_seq[i-1]['p_place'].float()\n",
    "\t\t\tp3 = action_seq[bIdx+1]['p_pick'].float()\n",
    "\t\t\tp4 = action_seq[i]['p_place'].float()\n",
    "\n",
    "\t\t\t# Current cost\n",
    "\t\t\tp = action_seq[bIdx]['p_place'].float()\n",
    "\t\t\tmin_cost = torch.norm(p1 - p)\n",
    "\t\t\tmin_cost += torch.norm(p - p3)\n",
    "\t\t\tmin_cost += torch.norm(p2 - p)\n",
    "\t\t\tmin_cost += torch.norm(p - p4)\n",
    "\t\t\tmin_cost = (min_cost * env.normalization_factor).item()\n",
    "\n",
    "\t\t\t# Find the best buffer\n",
    "\t\t\tbest_p = None\n",
    "\t\t\tfor p in P:\n",
    "\t\t\t\tcost = torch.norm(p1 - p)\n",
    "\t\t\t\tcost += torch.norm(p - p3)\n",
    "\t\t\t\tcost += torch.norm(p2 - p)\n",
    "\t\t\t\tcost += torch.norm(p - p4)\n",
    "\t\t\t\tcost = (cost * env.normalization_factor).item()\n",
    "\t\t\t\tif cost < min_cost:\n",
    "\t\t\t\t\tbest_p = p.clone().to(torch.long)\n",
    "\t\t\t\t\tmin_cost = cost\n",
    "\n",
    "\t\t\t# Update the best buffer\n",
    "\t\t\tif best_p is not None:\n",
    "\t\t\t\tif verbose > 0:\n",
    "\t\t\t\t\tlast_pos = action_seq[bIdx]['p_place']\n",
    "\t\t\t\t\tprint(f'Buffer of obj {k} changed from pos {last_pos.tolist()} to pos {best_p.tolist()}')\n",
    "\n",
    "\t\t\t\taction_seq[bIdx] = {\n",
    "\t\t\t\t\t'k': k,\n",
    "\t\t\t\t\t'p_pick': action_seq[bIdx]['p_pick'],\n",
    "\t\t\t\t\t'p_place': best_p\n",
    "\t\t\t\t}\n",
    "\t\t\t\tif torch.equal(best_p, action_seq[i]['p_place']):\n",
    "\t\t\t\t\tif verbose > 0:\n",
    "\t\t\t\t\t\tprint(f'New static buffer is the same as the current one, so the action {i} is removed')\n",
    "\t\t\t\t\tdel action_seq[i]\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\taction_seq[i] = {\n",
    "\t\t\t\t\t\t'k': k,\n",
    "\t\t\t\t\t\t'p_pick': best_p,\n",
    "\t\t\t\t\t\t'p_place': action_seq[i]['p_place']\n",
    "\t\t\t\t\t}\n",
    "\t\t\t\tbreak\n",
    "\n",
    "\t\tenv.step(plan[i])\n",
    "\t\tH[i+1] = env.get_state()\n",
    "\t\tB[k] = i\n",
    "\n",
    "\t# Generate the refined plan\n",
    "\trefined_plan = [env.encode_move(a['k'], a['p_place']) for a in action_seq]\n",
    "\treturn refined_plan\n",
    "\n",
    "def plan_refinement_stack(env, plan, initial_scene, target_scene, verbose=0):\n",
    "\tenv.static_stack = False\n",
    "\t# --- Stage 1: Immediate Redundancy Removal ---\n",
    "\tdecoded_plan = []\n",
    "\tfor action in plan:\n",
    "\t\tdecoded_action = (action, env.decode_action(action))\n",
    "\t\t# each entry: (action, (action_type, start_obj, target_obj, coord))\n",
    "\t\t# remove redundant action on the latest manipulated object\n",
    "\t\t# This check is for consecutive actions on the SAME object.\n",
    "\t\t# It keeps the last action on that object and discards previous consecutive ones.\n",
    "\t\tif len(decoded_plan) > 0 and decoded_action[1][1] == decoded_plan[-1][1][1]:\n",
    "\t\t\tif verbose > 0:\n",
    "\t\t\t\tif decoded_plan[-1][1][0] == 'move':\n",
    "\t\t\t\t\tprint(f'Redundant action, obj {decoded_plan[-1][1][1]} moving to {decoded_plan[-1][1][3]}, was removed')\n",
    "\t\t\t\telif decoded_plan[-1][1][0] == 'stack':\n",
    "\t\t\t\t\tprint(f'Redundant action, obj {decoded_plan[-1][1][1]} stacking on {decoded_plan[-1][1][2]}, was removed')\n",
    "\t\t\tdecoded_plan[-1] = decoded_action\n",
    "\t\telse:\n",
    "\t\t\tdecoded_plan.append(decoded_action)\n",
    "\n",
    "\t# --- Stage 2: State-Checking Redundancy (Simulate and Filter) ---\n",
    "\t# This stage is only required after the immediate redundancy removal\n",
    "\tenv.reset(initial_scene, target_scene)\n",
    "\taction_seq = []\n",
    "\tplan = []\n",
    "\n",
    "\tfor decoded_action in decoded_plan:\n",
    "\t\taction = decoded_action[0]\n",
    "\t\taction_type, start_obj, target_obj, coordinates = decoded_action[1]\n",
    "\t\t\n",
    "\t\tif action_type == 'stack':\n",
    "\t\t\t# If the object is already stacked, skip this action\n",
    "\t\t\tif env.current_x[start_obj, Indices.RELATION.start + target_obj] == 1:\n",
    "\t\t\t\tif verbose > 0:\n",
    "\t\t\t\t\tprint(f\"Skipping Stack action (obj {start_obj} -> {target_obj}): Object already stacked on target.\")\n",
    "\t\t\t\tcontinue\n",
    "\t\telif action_type == 'move':\n",
    "\t\t\t# If the object is ALREADY at the target coordinates for this move action.\n",
    "\t\t\tcurrent_coord = env.current_x[start_obj, Indices.COORD]\n",
    "\t\t\tif torch.equal(current_coord, coordinates):\n",
    "\t\t\t\tif verbose > 0:\n",
    "\t\t\t\t\tprint(f\"Skipping Move action (obj {start_obj} to {coordinates.tolist()}): Already at target position.\")\n",
    "\t\t\t\tcontinue\n",
    "\n",
    "\t\tif action_type == 'stack':\n",
    "\t\t\tp_place = env.current_x[target_obj, Indices.COORD].clone()\n",
    "\t\telse:\n",
    "\t\t\tp_place = coordinates.clone()\n",
    "\t\tp_pick = env.current_x[start_obj, Indices.COORD].clone()\n",
    "\t\tplan.append(action)\n",
    "\t\taction_seq.append({\n",
    "\t\t\t'type': action_type,\n",
    "\t\t\t'k': start_obj,\n",
    "\t\t\t'l': target_obj,\n",
    "\t\t\t'p_pick': p_pick,\n",
    "\t\t\t'p_place': p_place\n",
    "\t\t})\n",
    "\n",
    "\t\tenv._step(action_type, start_obj, target_obj, coordinates)\n",
    "\n",
    "\tenv.reset(initial_scene, target_scene)\n",
    "\tB = {}\n",
    "\tH = {0: env.get_state()}\t\t# arrangement history\n",
    "\tfor i in range(len(action_seq)):\n",
    "\t\tk = action_seq[i]['k']\n",
    "\n",
    "\t\tif k in B:\t\t\t# if object k was moved\n",
    "\t\t\tbIdx = B[k]\t\t# previous action index on k\n",
    "\n",
    "\t\t\t# Valid mask form the action index bIdx to i-1\n",
    "\t\t\tvalid_mask = torch.ones(env.grid_size, dtype=torch.bool)\n",
    "\t\t\tfor j in range(bIdx, i+1):\n",
    "\t\t\t\tenv.set_state(copy_state(H[j]))\n",
    "\t\t\t\tvalid_mask &= env.valid_center_mask(k)\n",
    "\n",
    "\t\t\tenv.set_state(copy_state(H[i]))\n",
    "\n",
    "\t\t\tP = valid_mask.nonzero(as_tuple=False).float()\n",
    "\t\t\tif len(P) == 0:\n",
    "\t\t\t\tif verbose > 0:\n",
    "\t\t\t\t\tprint(f'No feasible buffer set')\n",
    "\n",
    "\t\t\t# Occupied moving buffers in the action index bIdx to i-1\n",
    "\t\t\tempty_objs = []\n",
    "\t\t\tstable_j = env.stability_mask[k]\n",
    "\t\t\tcandidates = torch.nonzero(stable_j, as_tuple=False).view(-1).tolist()\n",
    "\t\t\tfor obj in candidates:\n",
    "\t\t\t\t# if obj is empty in the whole bIdx to i-1 period\n",
    "\t\t\t\tis_empty = True\n",
    "\t\t\t\tfor j in range(bIdx, i+1):\n",
    "\t\t\t\t\tif get_object_above(H[j]['current'], obj) is not None:\n",
    "\t\t\t\t\t\tis_empty = False\n",
    "\t\t\t\t\t\tbreak\n",
    "\t\t\t\tif is_empty:\n",
    "\t\t\t\t\tempty_objs.append(obj)\n",
    "\n",
    "\t\t\t# Distances\n",
    "\t\t\tp1 = action_seq[bIdx]['p_pick'].float()\n",
    "\t\t\tp2 = action_seq[i-1]['p_place'].float()\n",
    "\t\t\tp3 = action_seq[bIdx+1]['p_pick'].float()\n",
    "\t\t\tp4 = action_seq[i]['p_place'].float()\n",
    "\n",
    "\t\t\t# Current cost\n",
    "\t\t\tif action_seq[bIdx]['type'] == 'stack':\n",
    "\t\t\t\tp_to_buff = action_seq[bIdx]['p_place'].float()\n",
    "\t\t\t\tp_i = H[i]['current'][k, Indices.COORD]\n",
    "\t\t\t\tmin_cost = torch.norm(p1 - p_to_buff)\n",
    "\t\t\t\tmin_cost += torch.norm(p_to_buff - p3)\n",
    "\t\t\t\tmin_cost += torch.norm(p2 - p_i)\n",
    "\t\t\t\tmin_cost += torch.norm(p_i - p4)\n",
    "\t\t\t\tmin_cost = (min_cost * env.normalization_factor).item()\n",
    "\t\t\telse:\n",
    "\t\t\t\tp = action_seq[bIdx]['p_place'].float()\n",
    "\t\t\t\tmin_cost = torch.norm(p1 - p)\n",
    "\t\t\t\tmin_cost += torch.norm(p - p3)\n",
    "\t\t\t\tmin_cost += torch.norm(p2 - p)\n",
    "\t\t\t\tmin_cost += torch.norm(p - p4)\n",
    "\t\t\t\tmin_cost = (min_cost * env.normalization_factor).item()\n",
    "\n",
    "\t\t\t# Find the best buffer\n",
    "\t\t\tbest_p = None\n",
    "\t\t\tfor p in P:\n",
    "\t\t\t\tcost = torch.norm(p1 - p)\n",
    "\t\t\t\tcost += torch.norm(p - p3)\n",
    "\t\t\t\tcost += torch.norm(p2 - p)\n",
    "\t\t\t\tcost += torch.norm(p - p4)\n",
    "\t\t\t\tcost = (cost * env.normalization_factor).item()\n",
    "\t\t\t\tif cost < min_cost:\n",
    "\t\t\t\t\tbest_p = p.clone().to(torch.long)\n",
    "\t\t\t\t\tmin_cost = cost\n",
    "\n",
    "\t\t\tbest_obj = None\n",
    "\t\t\tfor empty_obj in empty_objs:\n",
    "\t\t\t\tp_to_buff = H[bIdx]['current'][empty_obj, Indices.COORD].float()\n",
    "\t\t\t\tp_i = H[i]['current'][empty_obj, Indices.COORD].float()\n",
    "\t\t\t\tcost = torch.norm(p1 - p_to_buff)\n",
    "\t\t\t\tcost += torch.norm(p_to_buff - p3)\n",
    "\t\t\t\tcost += torch.norm(p2 - p_i)\n",
    "\t\t\t\tcost += torch.norm(p_i - p4)\n",
    "\t\t\t\tcost = (cost * env.normalization_factor).item()\n",
    "\t\t\t\tif cost < min_cost:\n",
    "\t\t\t\t\tmin_cost = cost\n",
    "\t\t\t\t\tbest_obj = empty_obj\n",
    "\n",
    "\t\t\t# Update the best buffer\n",
    "\t\t\tif best_obj is not None:\n",
    "\t\t\t\tif verbose > 0:\n",
    "\t\t\t\t\tif action_seq[bIdx]['type'] == 'stack':\n",
    "\t\t\t\t\t\tlast_obj = action_seq[bIdx]['l']\n",
    "\t\t\t\t\t\tprint(f'Buffer of obj {k} changed from obj {last_obj} to obj {best_obj}')\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\tlast_pos = action_seq[bIdx]['p_place']\n",
    "\t\t\t\t\t\tprint(f'Buffer of obj {k} changed from pos {last_pos.tolist()} to obj {best_obj}')\n",
    "\n",
    "\t\t\t\taction_seq[bIdx] = {\n",
    "\t\t\t\t\t'type': 'stack',\n",
    "\t\t\t\t\t'k': k,\n",
    "\t\t\t\t\t'l': best_obj,\n",
    "\t\t\t\t\t'p_pick': action_seq[bIdx]['p_pick'].clone(),\n",
    "\t\t\t\t\t'p_place': H[bIdx]['current'][best_obj, Indices.COORD].clone(),\n",
    "\t\t\t\t}\n",
    "\t\t\t\tif action_seq[i]['type'] == 'stack' and action_seq[i]['l'] == best_obj:\n",
    "\t\t\t\t\tif verbose > 0:\n",
    "\t\t\t\t\t\tprint(f'New moving buffer is the same as the current one, so the action {i} is removed')\n",
    "\t\t\t\t\tdel action_seq[i]\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\taction_seq[i]['p_pick'] = H[i]['current'][best_obj, Indices.COORD].clone()\n",
    "\t\t\t\tbreak\n",
    "\t\t\telif best_p is not None:\n",
    "\t\t\t\tif verbose > 0:\n",
    "\t\t\t\t\tif action_seq[bIdx]['type'] == 'stack':\n",
    "\t\t\t\t\t\tlast_obj = action_seq[bIdx]['l']\n",
    "\t\t\t\t\t\tprint(f'Buffer of obj {k} changed from obj {last_obj} to pos {best_p.tolist()}')\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\tlast_pos = action_seq[bIdx]['p_place']\n",
    "\t\t\t\t\t\tprint(f'Buffer of obj {k} changed from pos {last_pos.tolist()} to pos {best_p.tolist()}')\n",
    "\n",
    "\t\t\t\taction_seq[bIdx] = {\n",
    "\t\t\t\t\t'type': 'move',\n",
    "\t\t\t\t\t'k': k,\n",
    "\t\t\t\t\t'l': k,\n",
    "\t\t\t\t\t'p_pick': action_seq[bIdx]['p_pick'].clone(),\n",
    "\t\t\t\t\t'p_place': best_p\n",
    "\t\t\t\t}\n",
    "\t\t\t\tif action_seq[i]['type'] == 'move' and torch.equal(best_p, action_seq[i]['p_place']):\n",
    "\t\t\t\t\tif verbose > 0:\n",
    "\t\t\t\t\t\tprint(f'New static buffer is the same as the current one, so the action {i} is removed')\n",
    "\t\t\t\t\tdel action_seq[i]\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\taction_seq[i]['p_pick'] = best_p\n",
    "\t\t\t\tbreak\n",
    "\n",
    "\t\tenv.step(plan[i])\n",
    "\t\tH[i+1] = env.get_state()\n",
    "\t\tB[k] = i\n",
    "\n",
    "\trefined_plan = []\n",
    "\tfor a in action_seq:\n",
    "\t\tif a['type'] == 'stack':\n",
    "\t\t\trefined_plan.append(env.encode_stack(a['k'], a['l']))\n",
    "\t\telse:\n",
    "\t\t\trefined_plan.append(env.encode_move(a['k'], a['p_place']))\n",
    "\n",
    "\treturn refined_plan\n",
    "\n",
    "# refine_until_convergence(env, plan, initial_scene, target_scene, 'Labbe', verbose=1);\n",
    "# print('-----')\n",
    "# refine_until_convergence(env, plan, initial_scene, target_scene, 'Labbe_S', verbose=1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scene_json_to_x(json_scene):\n",
    "\tinitial_x = torch.tensor([\n",
    "\t\t[obj['label'], *obj['size'], *obj['initial_pos'], *obj['initial_relation']]\n",
    "\t\tfor obj in json_scene['objects']\n",
    "\t], dtype=torch.float32)\n",
    "\n",
    "\ttarget_x = torch.tensor([\n",
    "\t\t[obj['label'], *obj['size'], *obj['target_pos'], *obj['target_relation']]\n",
    "\t\tfor obj in json_scene['objects']\n",
    "\t], dtype=torch.float32)\n",
    "\n",
    "\treturn initial_x, target_x\n",
    "\n",
    "def load_json_scenes(num_objects, grid_size, phi, num_scenes=None):\n",
    "\tscenes = []\n",
    "\tdir_path = f'scenes/phi_{phi}/g{grid_size[0]}.{grid_size[1]}/n{num_objects}'\n",
    "\n",
    "\tfor filename in os.listdir(dir_path):\n",
    "\t\tif not filename.endswith('.json'):\n",
    "\t\t\tcontinue\n",
    "\t\t\n",
    "\t\twith open(os.path.join(dir_path, filename), 'r') as f:\n",
    "\t\t\tscene = json.load(f)\n",
    "\t\t\n",
    "\t\tscenes.append(scene)\n",
    "\t\n",
    "\t# Sort the scenes by id\n",
    "\tscenes.sort(key=lambda x: x['scene_id'])\n",
    "\t\n",
    "\tif num_scenes is not None:\n",
    "\t\treturn scenes[:num_scenes]\n",
    "\treturn scenes\n",
    "\n",
    "def load_scenes(num_objects, grid_size, phi):\n",
    "\tscenes = []\n",
    "\n",
    "\tjson_scenes = load_json_scenes(num_objects, grid_size, phi)\n",
    "\tfor json_scene in json_scenes:\n",
    "\t\tinitial_x, target_x = scene_json_to_x(json_scene)\n",
    "\t\tscenes.append({\n",
    "\t\t\t'initial_scene': initial_x,\n",
    "\t\t\t'target_scene': target_x\n",
    "\t\t})\n",
    "\n",
    "\treturn scenes\n",
    "\n",
    "def save_runs(json_scenes, env, phi, alg, file_name, num_runs=1, **kwargs):\n",
    "\tdir_path = f'runs/phi_{phi}/{env.mode}/g{env.grid_size[0]}.{env.grid_size[1]}/n{env.N}'\n",
    "\tos.makedirs(dir_path, exist_ok=True)\n",
    "\t\n",
    "\tdata_path = os.path.join(dir_path, f'{file_name}.csv')\n",
    "\n",
    "\tfile_exists = os.path.exists(data_path)\n",
    "\tif file_exists:\n",
    "\t\tdf = pd.read_csv(data_path, converters={\n",
    "\t\t\t'plans': eval, 'steps': eval,\n",
    "\t\t\t'elapsed_times': eval, 'costs': eval\n",
    "\t\t})\n",
    "\telse:\n",
    "\t\tdf = pd.DataFrame()\n",
    "\n",
    "\tprint(f'----{alg.__name__}:{file_name}----')\n",
    "\ttotal_runs = len(json_scenes) * num_runs\n",
    "\tpbar = tqdm(total=total_runs, unit='run')\n",
    "\n",
    "\tfor scene_idx, json_scene in enumerate(json_scenes):\n",
    "\t\tscene_id = json_scene['scene_id']\n",
    "\t\talready_num_runs = 0\n",
    "\t\tremaining_num_runs = num_runs\n",
    "\t\tif file_exists and scene_id in df['scene_id'].values:\n",
    "\t\t\tidx = df.index[df['scene_id'] == scene_id][0]\n",
    "\t\t\talready_num_runs = len(df.at[idx, 'plans'])\n",
    "\t\t\tremaining_num_runs = num_runs - already_num_runs\n",
    "\t\t\tpbar.update(already_num_runs)\n",
    "\t\t\tif remaining_num_runs <= 0:\n",
    "\t\t\t\tpbar.set_description(f'Skipping scene {scene_id} - already solved {already_num_runs} times')\n",
    "\t\t\t\tcontinue\n",
    "\n",
    "\t\tfor run_idx in range(1, remaining_num_runs + 1):\n",
    "\t\t\tpbar.set_description(f'Scene {scene_idx}/{len(json_scenes)} - Run {run_idx+already_num_runs}/{num_runs}')\n",
    "\n",
    "\t\t\tinitial_x, target_x = scene_json_to_x(json_scene)\n",
    "\t\t\tinitial_x, target_x = initial_x.to(torch.long), target_x.to(torch.long)\n",
    "\t\t\tenv.reset(initial_x, target_x)\n",
    "\t\t\tplan, step, elapsed_time = alg(env).solve(**kwargs)\n",
    "\t\t\tcost = env_cost(env, plan, initial_x, target_x, log=False)\n",
    "\n",
    "\t\t\tif file_exists and scene_id in df['scene_id'].values:\n",
    "\t\t\t\tidx = df.index[df['scene_id'] == scene_id][0]\n",
    "\t\t\t\tdf.at[idx, 'plans'] += [plan]\n",
    "\t\t\t\tdf.at[idx, 'steps'] += [step]\n",
    "\t\t\t\tdf.at[idx, 'elapsed_times'] += [elapsed_time]\n",
    "\t\t\t\tdf.at[idx, 'costs'] += [cost]\n",
    "\t\t\telse:\n",
    "\t\t\t\tnew_row = pd.DataFrame([{\n",
    "\t\t\t\t\t'scene_id': scene_id,\n",
    "\t\t\t\t\t'mode': env.mode,\n",
    "\t\t\t\t\t'n': json_scene['num_objects'],\n",
    "\t\t\t\t\t'grid_size': json_scene['grid_size'],\n",
    "\t\t\t\t\t'alg': file_name,\n",
    "\t\t\t\t\t'plans': [plan],\n",
    "\t\t\t\t\t'steps': [step],\n",
    "\t\t\t\t\t'elapsed_times': [elapsed_time],\n",
    "\t\t\t\t\t'costs': [cost],\n",
    "\t\t\t\t}])\n",
    "\t\t\t\tdf = pd.concat([df, new_row], ignore_index=True)\n",
    "\n",
    "\t\t\t# Sort and save\n",
    "\t\t\tdf = df.sort_values(by='scene_id').reset_index(drop=True)\n",
    "\t\t\tdf.to_csv(data_path, index=False)\n",
    "\t\t\tfile_exists = True\n",
    "\n",
    "\t\t\tpbar.update(1)\n",
    "\t\tpbar.set_description(f'Scene {scene_idx}/{len(json_scenes)} - Run {run_idx+already_num_runs}/{num_runs}')\n",
    "\t\t\n",
    "\tpbar.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_(num_runs, mode, num_objects, grid_size, phi, time_limit):\n",
    "    num_buffers = 4\n",
    "    env = SceneManager(\n",
    "        mode=mode, num_objects=num_objects, \n",
    "        grid_size=grid_size, phi=phi, \n",
    "        terminal_cost=True, verbose=0\n",
    "    )\n",
    "    json_scenes = load_json_scenes(num_objects, grid_size, phi, num_scenes=10)\n",
    "    save_runs(json_scenes, env, phi, Labbe, \"Labbe\", num_runs=num_runs, c=0.1, time_limit=time_limit)\n",
    "    save_runs(json_scenes, env, phi, Labbe_S, \"Labbe+S\", num_runs=num_runs, c=0.1, time_limit=time_limit)\n",
    "    save_runs(json_scenes, env, phi, StrapGA, f\"StrapGA_{num_buffers}b\", num_runs=num_runs, num_buffers=num_buffers, time_limit=time_limit)\n",
    "    save_runs(json_scenes, env, phi, StrapGA_S, f\"StrapGA+S_{num_buffers}b\", num_runs=num_runs, num_buffers=num_buffers, time_limit=time_limit)\n",
    "    # save_runs(json_scenes, env, phi, Sorp, f\"Sorp.5_{num_buffers}b\", num_runs=num_runs, num_buffers=num_buffers, c=0.5, time_limit=time_limit)\n",
    "    # save_runs(json_scenes, env, phi, Strap, \"Strap\", num_runs=num_runs, num_buffers=num_buffers, time_limit=time_limit)\n",
    "    # save_runs(json_scenes, env, phi, Strap_S, \"Strap_S\", num_runs=num_runs, num_buffers=num_buffers, time_limit=time_limit)\n",
    "\n",
    "phi = 0.2\n",
    "num_runs = 1\n",
    "time_limit = 360\n",
    "grid_size = (101, 101)\n",
    "mode = 'stationary'\n",
    "\n",
    "run_(num_runs=num_runs, mode=mode, num_objects=3, grid_size=grid_size, phi=phi, time_limit=time_limit)\n",
    "run_(num_runs=num_runs, mode=mode, num_objects=4, grid_size=grid_size, phi=phi, time_limit=time_limit)\n",
    "run_(num_runs=num_runs, mode=mode, num_objects=5, grid_size=grid_size, phi=phi, time_limit=time_limit)\n",
    "run_(num_runs=num_runs, mode=mode, num_objects=6, grid_size=grid_size, phi=phi, time_limit=time_limit)\n",
    "run_(num_runs=num_runs, mode=mode, num_objects=7, grid_size=grid_size, phi=phi, time_limit=time_limit)\n",
    "run_(num_runs=num_runs, mode=mode, num_objects=8, grid_size=grid_size, phi=phi, time_limit=time_limit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas.api.types import CategoricalDtype\n",
    "from matplotlib import colormaps\n",
    "\n",
    "def load_runs(mode, grid_size, algs, n_values, phi, runs_dir='runs'):\n",
    "\tall_dfs = []\n",
    "\n",
    "\tfor num_objects in n_values:\n",
    "\t\tfor alg_name in algs:\n",
    "\t\t\tfilename = f'{runs_dir}/phi_{phi}/{mode}/g{grid_size[0]}.{grid_size[1]}/n{num_objects}/{alg_name}.csv'\n",
    "\t\t\tif not os.path.isfile(filename):\n",
    "\t\t\t\tcontinue\n",
    "\t\t\t\n",
    "\t\t\t# Read CSV\n",
    "\t\t\tdf = pd.read_csv(filename)\n",
    "\n",
    "\t\t\t# calculate the mean not-None costs\n",
    "\t\t\tdf['cost'] = df['costs'].apply(lambda x: np.mean([cost for cost in eval(x) if cost is not None]) if isinstance(eval(x), list) else np.nan)\n",
    "\t\t\tdf['step'] = df['steps'].apply(lambda x: np.mean([step for step in eval(x)]))\n",
    "\t\t\tdf['elapsed_time'] = df['elapsed_times'].apply(lambda x: np.mean([time for time in eval(x)]))\n",
    "\n",
    "\t\t\tall_dfs.append(df)\n",
    "\n",
    "\t# Merge all into one big DataFrame\n",
    "\tif all_dfs:\n",
    "\t\tmerged_df = pd.concat(all_dfs, ignore_index=True)\n",
    "\t\treturn merged_df\n",
    "\telse:\n",
    "\t\tprint(\"No valid run files found.\")\n",
    "\t\treturn pd.DataFrame()  # Return empty DF if nothing was loaded\n",
    "\n",
    "def compare_algs(df, figsize=(10, 4), std=False, sr=False, step=False, title=''):\n",
    "\tif df.empty:\n",
    "\t\tprint(\"No data to compare.\")\n",
    "\n",
    "\t# Filter only valid rows (cost not null = successful run)\n",
    "\tdf['success'] = df['cost'].notna()\n",
    "\n",
    "\t# Preserve algorithm order as they appear in the dataframe\n",
    "\talg_order = df['alg'].drop_duplicates().tolist()\n",
    "\talg_dtype = CategoricalDtype(categories=alg_order, ordered=True)\n",
    "\tdf['alg'] = df['alg'].astype(alg_dtype)\n",
    "\t\n",
    "\t# Group by n and alg\n",
    "\tgrouped = df.groupby(['n', 'alg'])\n",
    "\n",
    "\t# Aggregated metrics\n",
    "\tagg_df = grouped.agg(\n",
    "\t\tcost_mean=('cost', 'mean'),\n",
    "\t\tcost_std=('cost', 'std'),\n",
    "\t\tstep_mean=('step', 'mean'),\n",
    "\t\tstep_std=('step', 'std'),\n",
    "\t\ttime_mean=('elapsed_time', 'mean'),\n",
    "\t\ttime_std=('elapsed_time', 'std'),\n",
    "\t\tsuccess_rate=('success', lambda x: 100 * x.sum() / len(x))\n",
    "\t).reset_index()\n",
    "\n",
    "\t# Ensure algorithm column stays ordered\n",
    "\tagg_df['alg'] = agg_df['alg'].astype(alg_dtype)\n",
    "\n",
    "\t# Pivot with ordered columns\n",
    "\tcost_mean_pivot = agg_df.pivot(index='n', columns='alg', values='cost_mean')[alg_order]\n",
    "\tstep_mean_pivot = agg_df.pivot(index='n', columns='alg', values='step_mean')[alg_order]\n",
    "\ttime_mean_pivot = agg_df.pivot(index='n', columns='alg', values='time_mean')[alg_order]\n",
    "\n",
    "\tcost_std_pivot = agg_df.pivot(index='n', columns='alg', values='cost_std')[alg_order] if std else None\n",
    "\tstep_std_pivot = agg_df.pivot(index='n', columns='alg', values='step_std')[alg_order] if std else None\n",
    "\ttime_std_pivot = agg_df.pivot(index='n', columns='alg', values='time_std')[alg_order] if std else None\n",
    "\n",
    "\t# Plot\n",
    "\tsns.set_style('whitegrid')\n",
    "\tif sr and step:\n",
    "\t\tfig, axs = plt.subplots(1, 4, figsize=figsize)\n",
    "\telif sr or step:\n",
    "\t\tfig, axs = plt.subplots(1, 3, figsize=figsize)\n",
    "\telse:\n",
    "\t\tfig, axs = plt.subplots(1, 2, figsize=figsize)\n",
    "\n",
    "\tplot_bars(cost_mean_pivot, 'Cost Comparison', 'Cost', std_data=cost_std_pivot, ax=axs[0], cmap=colormaps['tab20b'])\n",
    "\tplot_bars(time_mean_pivot, 'Time Comparison', 'Time (log scale)', std_data=time_std_pivot, log_scale=True, ax=axs[1], cmap=colormaps['tab20b'])\n",
    "\tif sr and step:\n",
    "\t\tsr_pivot = agg_df.pivot(index='n', columns='alg', values='success_rate') if sr else None\n",
    "\t\tplot_bars(step_mean_pivot, 'Step Comparison', 'Step', std_data=step_std_pivot, ax=axs[2], cmap=colormaps['tab20b'])\n",
    "\t\tplot_bars(sr_pivot, 'Success Rate', 'Success Rate (%)', ax=axs[3], cmap=colormaps['tab20b'])\n",
    "\telif sr:\n",
    "\t\tsr_pivot = agg_df.pivot(index='n', columns='alg', values='success_rate') if sr else None\n",
    "\t\tplot_bars(sr_pivot, 'Success Rate', 'Success Rate (%)', ax=axs[2], cmap=colormaps['tab20b'])\n",
    "\telif step:\n",
    "\t\tplot_bars(step_mean_pivot, 'Step Comparison', 'Step', std_data=step_std_pivot, ax=axs[2], cmap=colormaps['tab20b'])\n",
    "\t\n",
    "\tfig.suptitle(title, fontsize=16)\n",
    "\tplt.tight_layout()\n",
    "\tplt.show()\n",
    "\n",
    "def plot_bars(df, title, ylabel, std_data=None, log_scale=False, ax=None, cmap=colormaps['tab20b']):\n",
    "\tif ax is None:\n",
    "\t\tfig, ax = plt.subplots(figsize=(6, 4))\n",
    "\n",
    "\tn_values = df.index.tolist()\n",
    "\talgs = df.columns.tolist()\n",
    "\tx = np.arange(len(n_values))\n",
    "\tbar_width = 0.1\n",
    "\n",
    "\tcolors = [cmap(i / len(algs)) for i in range(len(algs))]\n",
    "\n",
    "\tfor i, alg in enumerate(algs):\n",
    "\t\tvalues = df[alg].values\n",
    "\t\terrors = std_data[alg].values if std_data is not None and alg in std_data.columns else None\n",
    "\t\tlabel = alg\n",
    "\n",
    "\t\tax.bar(x + i * bar_width, values, width=bar_width, color=colors[i], label=label, yerr=errors, capsize=5 if errors is not None else 0)\n",
    "\n",
    "\tax.set_xlabel('Number of Objects (n)')\n",
    "\tax.set_ylabel(ylabel)\n",
    "\tax.set_title(title)\n",
    "\tax.set_xticks(x + bar_width * (len(algs) - 1) / 2)\n",
    "\tax.set_xticklabels(n_values)\n",
    "\tax.legend(title='Algorithm')\n",
    "\n",
    "\tif log_scale:\n",
    "\t\tax.set_yscale('log')\n",
    "\n",
    "\tax.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "phi = 0.2\n",
    "n_values = [3, 6]\n",
    "mode = 'stationary'\n",
    "\n",
    "algs = [\"Labbe\", \"Sorp.5_4b\", \"StrapGA_4b\", \"StrapGA+S_4b\", \"Sorp.5_6b\", \"StrapGA_6b\", \"StrapGA+S_6b\"]\n",
    "grid_size = (101, 101)\n",
    "df = load_runs(mode, grid_size, algs, n_values, phi, runs_dir='runs')\n",
    "compare_algs(df, step=True, figsize=(16, 4), title=f'{mode} | size = {grid_size} | φ = {phi}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_values = [3, 4, 5, 6, 7, 8]\n",
    "algs = [\"Labbe\", \"Sorp.5_4b\", \"StrapGA_4b\", \"StrapGA+S_4b\"]\n",
    "grid_size = (100, 100)\n",
    "df = load_runs(mode, grid_size, algs, n_values, phi, runs_dir='runs')\n",
    "compare_algs(df, step=True, figsize=(16, 4), title=f'{mode} | size = {grid_size} | φ = {phi}')\n",
    "n_values = [3, 4, 5, 6, 7, 8]\n",
    "algs = [\"Labbe\", \"Sorp.5_6b\", \"StrapGA_6b\", \"StrapGA+S_6b\"]\n",
    "grid_size = (100, 100)\n",
    "df = load_runs(mode, grid_size, algs, n_values, phi, runs_dir='runs')\n",
    "compare_algs(df, step=True, figsize=(16, 4), title=f'{mode} | size = {grid_size} | φ = {phi}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Scene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.env.scene_manager import cal_density\n",
    "\n",
    "def make_scenes(env, num_cases, use_stack=False, use_sides=False):\n",
    "\tscenes = []\n",
    "\tfor _ in range(num_cases):\n",
    "\t\tenv.reset(use_stack=use_stack, use_sides=use_sides)\n",
    "\t\tscenes.append({\n",
    "\t\t\t'initial_scene': env.initial_x,\n",
    "\t\t\t'target_scene': env.target_x\n",
    "\t\t})\n",
    "\t\n",
    "\treturn scenes\n",
    "\n",
    "def save_scenes(scenes, num_objects, grid_size, phi, verbose=0):\n",
    "\tdir_path = f'scenes/phi_{phi}/g{grid_size[0]}.{grid_size[1]}/n{num_objects}'\n",
    "\tos.makedirs(dir_path, exist_ok=True)\n",
    "\t\n",
    "\tfor scene in scenes:\n",
    "\t\tinitial_x = scene['initial_scene']\n",
    "\t\ttarget_x = scene['target_scene']\n",
    "\n",
    "\t\t# save the scene in a json file\n",
    "\t\tobjs = []\n",
    "\t\tfor obj in range(num_objects):\n",
    "\t\t\tobjs.append({\n",
    "\t\t\t\t'object_id': obj,\n",
    "\t\t\t\t'label': initial_x[obj, Indices.LABEL].item(),\n",
    "\t\t\t\t'size': initial_x[obj, Indices.SIZE].tolist(),\n",
    "\t\t\t\t'initial_pos': initial_x[obj, Indices.COORD].tolist(),\n",
    "\t\t\t\t'initial_relation': initial_x[obj, Indices.RELATION].tolist(),\n",
    "\t\t\t\t'target_pos': target_x[obj, Indices.COORD].tolist(),\n",
    "\t\t\t\t'target_relation': target_x[obj, Indices.RELATION].tolist(), \n",
    "\t\t\t})\n",
    "\t\t\n",
    "\t\t# Find the id of the current .json files in the dir_path\n",
    "\t\tfiles = os.listdir(dir_path)\n",
    "\t\tscene_id = 0\n",
    "\t\twhile f'scene_{scene_id:04d}.json' in files:\n",
    "\t\t\tscene_id += 1\n",
    "\n",
    "\t\t# create the json scene\n",
    "\t\tjson_scene = {\n",
    "\t\t\t'scene_id': scene_id,\n",
    "\t\t\t'phi': cal_density(initial_x, grid_size),\n",
    "\t\t\t'num_objects': num_objects,\n",
    "\t\t\t'grid_size': grid_size,\n",
    "\t\t\t'objects': objs\n",
    "\t\t}\n",
    "\n",
    "\t\t# Save the scene in a json file\n",
    "\t\tsubfolder_path = os.path.join(dir_path, f'scene_{scene_id:04d}.json')\n",
    "\t\twith open(subfolder_path, 'w') as f:\n",
    "\t\t\tjson.dump(json_scene, f, indent=4)\n",
    "\t\t\n",
    "\t\tif verbose > 0:\n",
    "\t\t\tprint(f'Saved {subfolder_path}')\n",
    "\n",
    "num_cases = 10\n",
    "n_values = [3, 4, 5, 6, 7, 8, 9]\n",
    "\n",
    "phi = 0.5\n",
    "use_sides = False\n",
    "grid_size = (100, 100)\n",
    "\n",
    "densities = {}\n",
    "for num_objects in n_values:\n",
    "\tprint(f'--n: {num_objects}--')\n",
    "\tenv = SceneManager(mode=\"stationary\", num_objects=num_objects, grid_size=grid_size, phi=phi)\n",
    "\tscenes = make_scenes(env, num_cases, use_sides=use_sides)\n",
    "\tsave_scenes(scenes, num_objects, grid_size, phi, verbose=1)\n",
    "\tdensities[num_objects] =[]\n",
    "\tfor i, scene in enumerate(scenes):\n",
    "\t\tdensities[num_objects].append(cal_density(scene['initial_scene'], grid_size))\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
